import pandas as pd
import numpy as np
import xgboost as xgb
import shap
from tqdm import tqdm
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, r2_score
from typing import Dict, List, Union
from src.core.load_config import settings
from src.services.ricequant_service import RiceQuantService
import joblib

class ContractAnalysisModel:
    """
    é€šç”¨åˆçº¦ä»·å€¼åˆ†ææ¨¡å‹ï¼ˆæ”¯æŒ5ç±»åˆçº¦ï¼‰

    ç‰¹ç‚¹ï¼š
    1. æ ¹æ®åˆçº¦ç±»å‹è‡ªåŠ¨é€‰æ‹©ç‰¹å¾
    2. é€‚é…å„ç±»åˆçº¦çš„ç‰¹æœ‰ä»·å€¼é©±åŠ¨å› ç´ 
    3. è¾“å‡ºå¯è§£é‡Šçš„ä»·å€¼è¯„åˆ†ï¼ˆ0-100ï¼‰
    """

    def __init__(self, contract_type: str):
        # åˆçº¦ç±»å‹ç‰¹å®šå‚æ•°
        self.contract_params = settings.mlModels.parameters

        # åˆçº¦ç±»å‹ç‰¹å®šä»·å€¼èŒƒå›´
        self.value_ranges = {
            'CS': (-0.03, 0.08),  # è‚¡ç¥¨ä»·å€¼èŒƒå›´ï¼ˆå¹´åŒ–ï¼‰
            'ETF': (-0.02, 0.06),  # ETFä»·å€¼èŒƒå›´
            'INDX': (-0.025, 0.07),  # æŒ‡æ•°ä»·å€¼èŒƒå›´
            'Future': (-0.04, 0.10),  # æœŸè´§ä»·å€¼èŒƒå›´
            'Option': (-0.05, 0.15)  # æœŸæƒä»·å€¼èŒƒå›´
        }

        self.model = None
        self.model_performace = None
        self.shap_explainer = None
        self.is_trained = False
        self.contract_type = contract_type
        self.value_features = None
        self.predict_days = None
        self.EARLY_STOPPING_ROUND = settings.mlModels.early_stopping_rounds

    def train(self, X, y):
        """
        è®­ç»ƒä»·å€¼åˆ†ææ¨¡å‹

        å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        contract_type: åˆçº¦ç±»å‹
        cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°

        è¿”å›:
        æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        """
        # 2. è®°å½•åˆçº¦ç±»å‹å’Œç‰¹å¾
        self.value_features = X.columns.tolist()

        # 3. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=settings.mlModels.cv_fold)
        cv_results = {
            'train_mae': [],
            'test_mae': [],
            'train_r2': [],
            'test_r2': [],
            'best_n_estimators': []
        }

        # 4. äº¤å‰éªŒè¯è®­ç»ƒ
        print('å¼€å§‹è®­ç»ƒæ¨¡å‹')
        for fold, (historical_idx, test_idx) in enumerate(tqdm(tscv.split(X))):
            X_historical, y_historical = X.iloc[historical_idx], y.iloc[historical_idx]

            N_historical = len(X_historical)
            VALID_RATIO = 0.2
            valid_size = int(N_historical * VALID_RATIO)

            # ä½¿ç”¨å®šä¹‰çš„å¸¸é‡ EARLY_STOPPING_ROUNDS
            if valid_size < self.EARLY_STOPPING_ROUND:
                print(f"Warning: Fold {fold + 1}: Validation set size ({valid_size}) is too small. Skipping fold.")
                continue

            train_subset_idx = N_historical - valid_size
            X_train = X_historical.iloc[:train_subset_idx]
            y_train = y_historical.iloc[:train_subset_idx]
            X_valid = X_historical.iloc[train_subset_idx:]
            y_valid = y_historical.iloc[train_subset_idx:]

            X_test = X.iloc[test_idx]
            y_test = y.iloc[test_idx]

            # è½¬æ¢ä¸º DMatrix æ ¼å¼
            dtrain = xgb.DMatrix(X_train, label=y_train)
            dvalid = xgb.DMatrix(X_valid, label=y_valid)
            dtest = xgb.DMatrix(X_test)

            eval_list = [(dtrain, 'train'), (dvalid, 'validation')]

            # ä½¿ç”¨ xgb.train è¿›è¡Œè®­ç»ƒï¼Œå‚æ•°å…¼å®¹æ€§æœ€é«˜
            bst = xgb.train(
                params=self.contract_params,  # åŒ…å« objective, learning_rate, eval_metric ç­‰
                dtrain=dtrain,
                num_boost_round=settings.mlModels.num_boost_rounds,
                evals=eval_list,
                early_stopping_rounds=self.EARLY_STOPPING_ROUND,
                verbose_eval=False
            )

            # è®°å½•æœ€ä½³è¿­ä»£æ¬¡æ•°
            best_n_estimators = bst.best_iteration
            cv_results['best_n_estimators'].append(best_n_estimators)

            # ä½¿ç”¨æœ€ä½³è¿­ä»£æ¬¡æ•°å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°
            y_train_pred = bst.predict(dtrain, iteration_range=(0, best_n_estimators))
            y_test_pred = bst.predict(dtest, iteration_range=(0, best_n_estimators))

            cv_results['train_mae'].append(mean_absolute_error(y_train, y_train_pred))
            cv_results['test_mae'].append(mean_absolute_error(y_test, y_test_pred))
            cv_results['train_r2'].append(r2_score(y_train, y_train_pred))
            cv_results['test_r2'].append(r2_score(y_test, y_test_pred))

        # è®¡ç®—æœ€ä½³è½®æ•°çš„å¹³å‡å€¼
        avg_best_n_estimators = int(np.mean(cv_results['best_n_estimators']))
        print(f"äº¤å‰éªŒè¯å¹³å‡æœ€ä½³è¿­ä»£æ¬¡æ•°: {avg_best_n_estimators}")

        # 4. ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å¹³å‡æœ€ä½³è½®æ•°ï¼‰
        # æœ€ç»ˆæ¨¡å‹ä½¿ç”¨ XGBRegressor å°è£…å™¨ï¼Œä¾¿äºåç»­é›†æˆï¼ˆä¾‹å¦‚ SHAPï¼‰
        print(f'ä½¿ç”¨å…¨éƒ¨æ•°æ®å’Œå¹³å‡æœ€ä½³è¿­ä»£æ¬¡æ•° {avg_best_n_estimators} é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...')
        final_params = self.contract_params.copy()
        final_params['n_estimators'] = avg_best_n_estimators
        final_params.pop('eval_metric', None)  # æœ€ç»ˆè®­ç»ƒæ— éœ€ç›‘æ§æŒ‡æ ‡
        self.model = xgb.XGBRegressor(**final_params)
        self.model.fit(X, y)

        # 5. åˆ›å»ºSHAPè§£é‡Šå™¨
        self.shap_explainer = shap.TreeExplainer(self.model)

        # 6. è®°å½•æ¨¡å‹çŠ¶æ€
        self.is_trained = True

        # 7. ä¿å­˜æ¨¡å‹æ€§èƒ½
        performance = {
            'train_mae': np.mean(cv_results['train_mae']),
            'test_mae': np.mean(cv_results['test_mae']),
            'train_r2': np.mean(cv_results['train_r2']),
            'test_r2': np.mean(cv_results['test_r2']),
            'avg_n_estimators': avg_best_n_estimators,  # å¢åŠ å¹³å‡æœ€ä½³è½®æ•°
            'sample_size': len(X)
        }
        self.model_performace = performance

    def predict_excess_return(self, features: pd.Series) -> float:
        """é¢„æµ‹æœªæ¥20æ—¥è¶…é¢æ”¶ç›Š"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        pred_data = features[self.value_features].values.reshape(1, -1)
        return self.model.predict(pred_data)[0]

    def predict_value_score(self, features: pd.Series) -> float:
        """é¢„æµ‹æŠ•èµ„ä»·å€¼è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰"""
        predicted_excess_returns = self.predict_excess_return(features)
        min_value, max_value = self.value_ranges[self.contract_type]

        # æ˜ å°„åˆ°0-100åˆ†ï¼Œä½¿ç”¨Sigmoidæˆ–Sigmoid-likeå‡½æ•°è¿›è¡Œå¹³æ»‘ï¼Œé¿å…ç®€å•çš„çº¿æ€§æˆªæ–­
        # ç®€å•çº¿æ€§æ˜ å°„ï¼ˆä¿æŒåŸæ ·ï¼Œä½†è¿›è¡Œäº†æˆªæ–­ï¼‰
        score = 100 * (predicted_excess_returns - min_value) / (max_value - min_value)
        return max(0, min(100, score))

    def get_value_rationale(self, features: pd.Series) -> List[Dict[str, Union[str, float]]]:
        """ç”Ÿæˆä»·å€¼è¯„åˆ†çš„ç†ç”±è¯´æ˜ï¼ˆåŸºäºSHAPå€¼ï¼‰"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        if self.shap_explainer is None:
            return []

        # 1. è®¡ç®—SHAPå€¼
        pred_data = features[self.value_features].values.reshape(1, -1)
        # æ³¨æ„ï¼šTreeExplainer.shap_values() çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ•°ç»„æˆ–åˆ—è¡¨
        shap_values = self.shap_explainer.shap_values(pred_data)

        if isinstance(shap_values, list):
            # For multi-output models, though usually just one for regression
            shap_values = shap_values[0]

            # 2. ç”Ÿæˆè§£é‡Š
        contributions: List[Dict[str, Union[str, float]]] = []
        for i, feature in enumerate(self.value_features):
            shap_value = shap_values[0][i] if shap_values.ndim == 2 else shap_values[i]

            # ä»…å…³æ³¨å¯¹é¢„æµ‹æœ‰æ˜¾è‘—å½±å“çš„ç‰¹å¾
            if abs(shap_value) < 0.005:
                continue

            explanation = self._get_feature_explanation(feature, shap_value, features)

            contributions.append({
                'feature': feature,
                'shap_value': float(shap_value),
                'explanation': explanation
            })

        # 3. æŒ‰ç»å¯¹SHAPå€¼æ’åº
        contributions.sort(key=lambda x: abs(x['shap_value']), reverse=True)
        return contributions

    def _get_feature_explanation(self, feature: str, shap_value: float, features: pd.Series) -> str:
        """
        ç”Ÿæˆæ›´ä¸“ä¸šã€æ›´å¯è§£é‡Šçš„ç‰¹å¾è´¡çŒ®è¯´æ˜
        """
        is_positive = shap_value > 0
        direction = "æ­£å‘" if is_positive else "è´Ÿå‘"

        # æå–å€¼
        value = features.get(feature, np.nan)
        if pd.isna(value):
            return f"ç‰¹å¾ {feature} æ•°æ®ç¼ºå¤±ï¼Œå¯¹ä»·å€¼äº§ç”Ÿäº† {direction} å½±å“ã€‚"

        # --- é€šç”¨ç‰¹å¾è§£é‡Š ---
        if feature == 'ma_20d':
            diff_percent = value * 100
            return f"ä»·æ ¼ç›¸å¯¹20æ—¥å‡çº¿ä¹–ç¦»åº¦ï¼ˆ{diff_percent:.2f}%ï¼‰å½±å“ï¼šä¹–ç¦»åº¦{'é«˜äº' if diff_percent > 0 else 'ä½äº'}é›¶å€¼ï¼Œæ¨¡å‹è§†ä¸º{direction}ä¿¡å·ã€‚"

        elif feature == 'vol_ratio_20_60':
            vol_ratio = value
            analysis = ""
            if vol_ratio > 1.1:
                analysis = "çŸ­æœŸæ³¢åŠ¨ç‡æ˜¾è‘—é«˜äºé•¿æœŸï¼Œé¢„ç¤ºå¸‚åœºè¿›å…¥é«˜æ³¢åŠ¨æˆ–è¶‹åŠ¿å¯èƒ½åè½¬ã€‚"
            elif vol_ratio < 0.9:
                analysis = "çŸ­æœŸæ³¢åŠ¨ç‡ä½äºé•¿æœŸï¼Œæ˜¾ç¤ºå¸‚åœºæƒ…ç»ªè¶‹äºç¨³å®šã€‚"
            else:
                analysis = "æ³¢åŠ¨ç‡ç»“æ„å¹³ç¨³ã€‚"
            return f"æ³¢åŠ¨ç‡æ–œç‡ ({vol_ratio:.2f}) åˆ†æï¼š{analysis}ï¼Œæ¨¡å‹è§†ä¸º{direction}ä¿¡å·ã€‚"

        elif feature == 'sharpe_20d':
            return f"å†å²20æ—¥å¤æ™®æ¯”ç‡ ({value:.3f})ï¼šä½“ç°è¿‘æœŸé£é™©è°ƒæ•´æ”¶ç›Šæ°´å¹³ï¼Œå¯¹ä»·å€¼æœ‰{direction}å½±å“ã€‚"

        elif feature == 'var_95':
            return f"60æ—¥å†å²VaR(95%) ({value * 100:.2f}%)ï¼šä½“ç°å°¾éƒ¨é£é™©æ°´å¹³ï¼Œé£é™©è¶Šä½å¯¹ä»·å€¼è¶Šæœ‰{direction}å½±å“ã€‚"

        # --- åˆçº¦ç±»å‹ç‰¹å®šè§£é‡Š ---
        elif self.contract_type == 'CS' and feature == 'turnover_ratio':
            ratio_percent = value * 100
            sentiment = "å¼ºåŠ¿èµ„é‡‘æµå…¥" if value > 1.5 else ("æ¸©å’Œæ´»è·ƒ" if value > 1.0 else "äº¤æŠ•æ¸…æ·¡")
            return f"æ¢æ‰‹ç‡ä¸å‡å€¼æ¯” ({ratio_percent:.0f}%)ï¼šå¸‚åœºæ´»è·ƒåº¦é«˜ï¼Œæ¨¡å‹æ•æ‰åˆ°{sentiment}å¸¦æ¥çš„{direction}å½±å“ã€‚"

        elif self.contract_type == 'Future' and feature == 'settlement':
            return f"å½“å‰ç»“ç®—ä»· ({value:.2f}) å¯¹ä»·å€¼é¢„æµ‹æœ‰{direction}å½±å“ã€‚"

        elif self.contract_type == 'Option' and feature == 'implied_vol':
            iv_percent = value * 100
            sentiment = "æº¢ä»·" if iv_percent > 30 else "ä½ä¼°"
            return f"éšå«æ³¢åŠ¨ç‡ ({iv_percent:.1f}%)ï¼šIVæ°´å¹³ç›¸å¯¹{'è¾ƒé«˜' if is_positive else 'è¾ƒä½'}ï¼Œå¸‚åœºæƒ…ç»ªå{sentiment}ï¼Œæ¨¡å‹è§†ä¸º{direction}ä¿¡å·ã€‚"

        # é»˜è®¤é€šç”¨è§£é‡Š
        return f"{feature} (å½“å‰å€¼: {value:.3f}) å¯¹æŠ•èµ„ä»·å€¼æœ‰ {direction} å½±å“ã€‚"

    def _analyze_risk_features(self, features: pd.Series) -> Dict[str, Union[str, float]]:
        """
        åŸºäºå…³é”®é£é™©ç‰¹å¾ï¼Œç»™å‡ºå®šæ€§é£é™©è¯„ä¼°
        """
        risk_level = "ä¸­"
        vol_20d = features.get('vol_20d', np.nan)
        var_95 = features.get('var_95', np.nan)
        cvar_95 = features.get('cvar_95', np.nan)

        # 1. æ³¢åŠ¨ç‡è¯„ä¼°
        if not pd.isna(vol_20d):
            if vol_20d > 0.4:
                risk_level = "é«˜"
            elif vol_20d < 0.15:
                if risk_level != "é«˜":  # ä¸è¦†ç›–é«˜é£é™©
                    risk_level = "ä½"

        # 2. å°¾éƒ¨é£é™©è¯„ä¼° (CVaRçš„ç»å¯¹å€¼è¶Šå¤§ï¼Œå°¾éƒ¨é£é™©è¶Šé«˜)
        if not pd.isna(cvar_95) and cvar_95 < -0.05:
            risk_level = "é«˜"  # å†å²æœ€å¤§æŸå¤±é£é™©å¤§ï¼Œç›´æ¥å®šä¸ºé«˜é£é™©

        return {
            "level": risk_level,
            "volatility": vol_20d,
            "var_95": var_95,
            "cvar_95": cvar_95
        }

    def generate_investment_report(self, features: pd.Series) -> str:
        """
        ç”ŸæˆåŒ…å«ä»·å€¼ã€é£é™©å’Œæ”¶ç›Šé¢„æµ‹çš„ç»¼åˆæŠ•èµ„åˆ†ææŠ¥å‘Š (Markdown æ ¼å¼)
        """
        # 1. æ ¸å¿ƒé¢„æµ‹
        value_score = self.predict_value_score(features)
        predicted_returns = self.predict_excess_return(features)

        # 2. é£é™©åˆ†æ
        risk_data = self._analyze_risk_features(features)

        # 3. ä»·å€¼ç†ç”±
        rationale = self.get_value_rationale(features)

        # 4. æŠ¥å‘Šç”Ÿæˆ
        report_markdown = []
        report_markdown.append(f"# åˆçº¦ä»·å€¼åˆ†ææŠ¥å‘Š - {self.contract_type}")
        report_markdown.append(f"## ğŸš€ æŠ•èµ„ä»·å€¼è¯„åˆ†ï¼š{value_score:.1f}/100")

        # ä»·å€¼ç­‰çº§åˆ¤æ–­
        if value_score >= 80:
            value_grade = "æå…·å¸å¼•åŠ› (Strong Buy)"
        elif value_score >= 60:
            value_grade = "ä¸­é«˜ä»·å€¼ (Buy)"
        elif value_score >= 40:
            value_grade = "ä¸­æ€§åå¤š (Hold)"
        else:
            value_grade = "ä½ä»·å€¼/é«˜ä¼° (Sell)"

        report_markdown.append(f"**è¯„ä¼°ç»“è®º:** **{value_grade}**")
        report_markdown.append("\n---")

        report_markdown.append("## ğŸ“Š æŠ•èµ„æ”¶ç›Šé¢„æµ‹")
        report_markdown.append(f"åŸºäºæ¨¡å‹é¢„æµ‹ï¼Œæœªæ¥20æ—¥è¶…é¢æ”¶ç›Šç‡ï¼ˆå¹´åŒ–ï¼‰é¢„æœŸä¸º: **{predicted_returns * 100:.2f}%**")
        report_markdown.append("\n---")

        report_markdown.append("## ğŸ›¡ï¸ é£é™©åˆ†æ (Investment Risk)")
        report_markdown.append(f"**å½“å‰é£é™©æ°´å¹³ï¼š** **{risk_data['level']}**")

        report_markdown.append("### é£é™©æŒ‡æ ‡å¿«ç…§ï¼š")
        report_markdown.append(f"- **20æ—¥æ³¢åŠ¨ç‡ (Volatility):** {risk_data['volatility']:.3f} (åæ˜ çŸ­æœŸä»·æ ¼éœ‡è¡ç¨‹åº¦)")
        report_markdown.append(f"- **VaR 95% (æœ€å¤§äºæŸ):** {risk_data['var_95'] * 100:.2f}% (60æ—¥å†å²æ•°æ®ï¼Œ95%ç½®ä¿¡åº¦ä¸‹çš„æœ€å¤§äºæŸ)")
        report_markdown.append(
            f"- **CVaR 95% (å¹³å‡å°¾éƒ¨äºæŸ):** {risk_data['cvar_95'] * 100:.2f}% (95%ç½®ä¿¡åº¦ä¸‹å¹³å‡æœ€å·®æŸå¤±ï¼Œ**å°¾éƒ¨é£é™©å…³é”®æŒ‡æ ‡**)")
        report_markdown.append("\n---")

        report_markdown.append("## ğŸ’¡ ä»·å€¼é©±åŠ¨å› ç´  (SHAP è§£é‡Š)")
        report_markdown.append("ä»¥ä¸‹æ˜¯æ¨¡å‹é¢„æµ‹è¯¥ä»·å€¼è¯„åˆ†çš„**ä¸»è¦åŸå› **ï¼ˆæŒ‰å½±å“åŠ›æ’åºï¼‰ï¼š")

        for item in rationale:
            impact = "ï¼ˆç§¯æè´¡çŒ®ï¼‰" if item['shap_value'] > 0 else "ï¼ˆæ¶ˆæè´¡çŒ®ï¼‰"
            report_markdown.append(f"- **{item['feature']}** {impact}: {item['explanation']}")

        report_markdown.append("\n---")
        report_markdown.append(f"æ¨¡å‹ç”± XGBoost è®­ç»ƒï¼Œç›®æ ‡å˜é‡ä¸ºæœªæ¥{self.predict_days}æ—¥è¶…é¢æ”¶ç›Šã€‚")

        return "\n".join(report_markdown)

    def preprocess_features_data(self, features_data: pd.DataFrame, start_date:str, end_date:str, shibor_type:str, predict_days:int):
        """
        é€šç”¨æ•°æ®é¢„å¤„ç†å‡½æ•°

        Args:
            features_data (pd.DataFrame): åŸå§‹ç‰¹å¾æ•°æ®
            start_date (str): å¼€å§‹æ—¥æœŸ
            end_date (str): ç»“æŸæ—¥æœŸ
            shibor_type (str): Shiboråˆ©ç‡ç±»å‹ï¼Œå¦‚'1W'
            predict_days (int): é¢„æµ‹å¤©æ•°

        Returns:
            tuple: (X_train, y_train) å¤„ç†åçš„ç‰¹å¾å’Œç›®æ ‡å˜é‡
        """
        self.predict_days = predict_days

        # åˆ é™¤æ‰€æœ‰å…¨é›¶åˆ—
        features_data = features_data.loc[:, ~(features_data == 0).all(axis=0)]

        # å¤åˆ¶æ•°æ®
        data = features_data.copy()
        x_col = data.columns.to_list()

        # ç¡®ä¿æŒ‰idå’Œæ—¥æœŸæ’åº
        features_data_sorted = features_data.sort_values(['order_book_id', 'date'])

        # è®¡ç®—æœªæ¥æ”¶ç›Šï¼ˆshiftçš„å¤©æ•°ä¸predict_daysç›¸å…³ï¼‰
        data['future_returns'] = features_data_sorted.groupby('order_book_id')['close'].transform(
            lambda x: x.shift(-predict_days) / x - 1
        )

        # åˆ†ç»„ç¼©å°¾æ“ä½œï¼ˆWinsorizationï¼‰
        def winsorize_series(series, lower_percentile=0.05, upper_percentile=0.95):
            lower_bound = series.quantile(lower_percentile)
            upper_bound = series.quantile(upper_percentile)
            return series.clip(lower=lower_bound, upper=upper_bound)
        data['future_returns'] = data.groupby('order_book_id')['future_returns'].transform(winsorize_series)

        # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡å˜é‡ï¼ˆéœ€è¦rice_quant_serviceå®ä¾‹ï¼‰
        rice_quant_service = RiceQuantService()
        data = rice_quant_service.merge_shibor_data(data, start_date, end_date, [shibor_type], predict_days)
        data['excess_returns'] = data['future_returns'] - data[shibor_type]

        # åˆ é™¤ä»»ä½•åŒ…å«NaNçš„è¡Œ
        data = data.dropna()

        # åˆ†ç¦»Xå’Œy
        X_train = data[x_col].set_index(['date', 'order_book_id'])
        y_train = data[['date', 'order_book_id', 'excess_returns']].set_index(['date', 'order_book_id'])

        return X_train, y_train

    def save_model(self, file_path: str):
        """
        ä¿å­˜æ¨¡å‹å®ä¾‹åˆ°æœ¬åœ°æ–‡ä»¶
        """
        joblib.dump(self, file_path)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {file_path}")

    @classmethod
    def load_model(cls, file_path: str):
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ¨¡å‹å®ä¾‹ï¼Œæ­¤ä¸ºç±»æ–¹æ³•ï¼Œå¯é€šè¿‡ç±»åç§°ç›´æ¥è°ƒç”¨
        """
        model_instance = joblib.load(file_path)
        return model_instance


if __name__ == '__main__':
    # 1. åˆå§‹åŒ–æ¨¡å‹
    value_model = ContractAnalysisModel('CS')
    features_data = pd.read_csv(r"/root/nas-private/bigdata_final_project/data/processed/20240401_20251128_3d96b3a4bf_CS_features_data.csv")

    # features_data = features_data.loc[:, ~(features_data == 0).all(axis=0)]   # åˆ é™¤æ‰€æœ‰å…¨é›¶åˆ—
    #
    # # 3. å®šä¹‰ç›®æ ‡å˜é‡ y (æœªæ¥20æ—¥è¶…é¢æ”¶ç›Š)
    # # ç›´æ¥åœ¨åŸå§‹æ•°æ®ä¸Šæ“ä½œï¼Œä¿æŒé¡ºåºä¸å˜
    # data = features_data.copy()
    # x_col = data.columns.to_list()
    # features_data_sorted = features_data.sort_values(['order_book_id', 'date'])  # ç¡®ä¿æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    # data['future_returns'] = features_data_sorted.groupby('order_book_id')['close'].transform(lambda x: x.shift(-5) / x - 1)
    #
    # # åˆ†ç»„ç¼©å°¾æ“ä½œï¼ˆWinsorizationï¼‰
    # def winsorize_series(series, lower_percentile=0.05, upper_percentile=0.95):
    #     lower_bound = series.quantile(lower_percentile)
    #     upper_bound = series.quantile(upper_percentile)
    #     return series.clip(lower=lower_bound, upper=upper_bound)
    # data['future_returns'] = data.groupby('order_book_id')['future_returns'].transform(winsorize_series)
    #
    # # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡å˜é‡
    # data = rice_quant_service.merge_shibor_data(data, '20240401', '20251128', ['1W'], 3)
    # data['excess_returns'] = data['future_returns'] - data['1W']
    #
    # # åˆ é™¤ä»»ä½•åŒ…å«NaNçš„è¡Œ
    # data = data.dropna()
    #
    # # åˆ†ç¦»Xå’Œy
    # X_train = data[x_col].set_index(['date', 'order_book_id'])
    # y_train = data[['date', 'order_book_id', 'excess_returns']]
    # y_train = y_train.set_index(['date', 'order_book_id'])

    # 4. è®­ç»ƒæ¨¡å‹
    X_train, y_train = value_model.preprocess_features_data(features_data, '20240401', '20251128', '1W', 3)
    value_model.train(X_train, y_train)
    print("\n--- æ¨¡å‹è®­ç»ƒæ€§èƒ½ ---")
    print(value_model.model_performace)
    print("---------------------------------")

    # 4. é¢„æµ‹å¹¶ç”ŸæˆæŠ¥å‘Š
    latest_features = X_train.iloc[-1].copy()  # è·å–æœ€æ–°ä¸€è¡Œæ•°æ® (Series)
    report = value_model.generate_investment_report(latest_features)

    print("\n--- ğŸ“ æœ€æ–°æŠ•èµ„åˆ†ææŠ¥å‘Š ---")
    print(report)
    print("-----------------------------")