import os
import pandas as pd
import numpy as np
from src.services.ricequant_service import RiceQuantService
from src.core.load_config import settings
import hashlib
from pykalman import KalmanFilter
from scipy import stats
from tqdm import tqdm

class MLService:
    """
    MLService å›Šæ‹¬ä»·å€¼åˆ†ææ¨¡å‹ã€é£é™©åˆ†ææ¨¡å‹ã€æ”¶ç›Šé¢„æµ‹æ¨¡å‹ï¼Œèƒ½è°ƒç”¨æŠ•èµ„å»ºè®®å¼•æ“åˆ†ææ¨¡å‹ç»“æœ
    """
    def __init__(self):
        self.ricequant_service = RiceQuantService()
        self.project_path = settings.project.project_dir
        self.features_data_path = os.path.join(self.project_path, settings.paths.processed_data)
        print("åˆå§‹åŒ– MLService æˆåŠŸï¼")

    def _analyze_CS(self, start_date:int, end_date:int, order_book_id_list: list = None):
        """
        å¯¹è‚¡ç¥¨æ—¥çº¿æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æ
        :param start_date: yyyymmddï¼Œintå‹
        :param end_date: yyyymmddï¼Œintå‹
        :return: åŒ…å«æ‰€æœ‰å…³é”®æŒ‡æ ‡çš„dictåˆ—è¡¨
        """
        cs_features_list = ['open', 'close', 'high', 'low', 'limit_up', 'limit_down', 'total_turnover', 'volume', 'num_trades', 'prev_close']
        df = self.ricequant_service.instruments_data_fetching(type='CS', start_date=start_date, end_date=end_date, features_list=cs_features_list, order_book_id_list=order_book_id_list)

        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®å¹¶æ’åº
        df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
        df = df.sort_values(['order_book_id', 'date']).reset_index(drop=True)

        # è®¡ç®—åŸºç¡€æŒ‡æ ‡ (å¤„ç†é™¤é›¶é”™è¯¯)
        print('è®¡ç®—åŸºç¡€æŒ‡æ ‡')
        df['institution_participation'] = np.where(
            df['num_trades'] > 0,
            df['volume'] / df['num_trades'],
            np.nan
        )

        # ä»£ç†æ—©æœŸæµåŠ¨æ€§ (2021-06-25å‰)
        print('è®¡ç®—ä»£ç†æ—©æœŸæµåŠ¨æ€§')
        price_range = (df['high'] - df['low']).replace(0, np.nan)
        df['volume_range_ratio'] = df['volume'] / price_range
        df['institution_participation'] = np.where(
            (df['date'] < '2021-06-25') | df['num_trades'].isna(),
            df['volume_range_ratio'],
            df['institution_participation']
        )

        # æµåŠ¨æ€§æ¯ç«­æŒ‡æ•° (å¤„ç†ä»·æ ¼èŒƒå›´ä¸ºé›¶)
        print('è®¡ç®—æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°')
        df['liquidity_dryup'] = np.where(
            price_range.notna(),
            ((df['limit_up'] - df['close']) / price_range) +
            ((df['close'] - df['limit_down']) / price_range),
            np.nan
        )

        # æ¶¨åœå»¶ç»­ç‡ (è¿ç»­æ¶¨åœå¤©æ•°)
        print('è®¡ç®—æ¶¨åœå»¶ç»­ç‡')
        df['is_limit_up'] = df['close'] >= df['limit_up'] * 0.995  # å®¹å¿0.5%è¯¯å·®
        df['consecutive_limit_up'] = 0
        for i in range(1, len(df)):
            if df.iloc[i]['is_limit_up']:
                prev_consec = df.iloc[i - 1]['consecutive_limit_up']
                df.iloc[i, df.columns.get_loc('consecutive_limit_up')] = prev_consec + 1
            else:
                df.iloc[i, df.columns.get_loc('consecutive_limit_up')] = 0

        # ========================
        # æ ¸å¿ƒï¼šæ»šåŠ¨çª—å£ TVP-SSM åˆ†æ
        # ========================
        print('åˆ©ç”¨TVP-SSMæŒ–æ˜å¸‚åœºæ·±å±‚çš„åŠ¨æ€é£é™©ç»“æ„')
        results = []
        window_size = 30  # æ»šåŠ¨çª—å£å¤§å°

        for order_book_id, group in tqdm(df.groupby('order_book_id')):
            group = group.sort_values('date').copy().reset_index(drop=True)
            n = len(group)

            # å‡†å¤‡æ—¶é—´åºåˆ—
            returns = (group['close'] / group['prev_close'] - 1).values
            liquidity_dryup = group['liquidity_dryup'].fillna(0.5).values

            # å­˜å‚¨åŠ¨æ€ä¼°è®¡ç»“æœ
            risk_premium = np.full(n, np.nan)
            liquidity_impact = np.full(n, np.nan)

            # åªæœ‰å½“æ•°æ®è¶³å¤Ÿé•¿æ—¶æ‰è¿›è¡Œæ»šåŠ¨ä¼°è®¡
            if n >= window_size:
                for t in range(window_size, n):
                    # æå–çª—å£å†…æ•°æ®
                    window_returns = returns[t - window_size:t]  # shape: (30,)
                    window_liquidity = liquidity_dryup[t - window_size:t]  # shape: (30,)

                    # æ„å»ºè§‚æµ‹çŸ©é˜µï¼šæ¯è¡Œ [1, liquidity_dryup_t]
                    obs_mat = np.column_stack([np.ones(window_size), window_liquidity])

                    try:
                        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆçŠ¶æ€ï¼š[é£é™©æº¢ä»·, æµåŠ¨æ€§ç³»æ•°]ï¼‰
                        kf = KalmanFilter(
                            transition_matrices=np.eye(2),  # çŠ¶æ€è½¬ç§»
                            observation_matrices=obs_mat,  # è§‚æµ‹çŸ©é˜µï¼ˆéšæ—¶é—´å˜åŒ–ï¼‰
                            initial_state_mean=[0, 0],
                            initial_state_covariance=np.eye(2),
                            observation_covariance=1e-3,
                            transition_covariance=np.eye(2) * 1e-4
                        )

                        # ä½¿ç”¨çª—å£å†…æ•°æ®æ»¤æ³¢
                        filtered_state_means, _ = kf.filter(window_returns)
                        # å–æœ€åä¸€å¤©çš„ä¼°è®¡å€¼ä½œä¸ºå½“å‰çŠ¶æ€
                        risk_premium[t] = filtered_state_means[-1, 0]
                        liquidity_impact[t] = filtered_state_means[-1, 1]

                    except Exception as e:
                        # æ•è·ä»»ä½•æ•°å€¼ä¸ç¨³å®šæˆ–SVDåˆ†è§£å¤±è´¥ç­‰é—®é¢˜
                        risk_premium[t] = np.nan
                        liquidity_impact[t] = np.nan

            # ========================
            # å°¾éƒ¨é£é™©æ¦‚ç‡
            # ========================
            tail_risk_prob = np.zeros(n)
            for t in range(window_size + 30, n):  # å‰é¢ç•™å‡ºç¼“å†²
                recent_rp = risk_premium[t - 60:t]  # æœ€è¿‘60å¤©
                valid_rp = recent_rp[~np.isnan(recent_rp)]
                if len(valid_rp) >= 20:
                    threshold_90 = np.percentile(valid_rp, 90)
                    current_liq = group.iloc[t]['liquidity_dryup']
                    current_rp = risk_premium[t]
                    if not np.isnan(current_rp) and current_liq > 0.8 and current_rp > threshold_90:
                        tail_risk_prob[t] = 0.65

            # ========================
            # æƒ…ç»ª-æµåŠ¨æ€§åŒ¹é…åº¦
            # ========================
            rolling_mean_ip = group['institution_participation'].rolling(30, min_periods=10).mean()
            emotion_liquidity_match = (
                    (group['institution_participation'] - rolling_mean_ip) *
                    (1 - group['liquidity_dryup'].fillna(0.5))
            )

            # ========================
            # æ„å»ºæ¯æ—¥ç»“æœ
            # ========================
            for i in range(n):
                risk_alert = False
                if (not np.isnan(emotion_liquidity_match.iloc[i]) and
                        group.iloc[i]['liquidity_dryup'] > 0.8 and
                        group.iloc[i]['consecutive_limit_up'] > 3 and
                        emotion_liquidity_match.iloc[i] < -0.3):
                    risk_alert = True

                results.append({
                    "date": group.iloc[i]['date'].strftime('%Y-%m-%d'),
                    "order_book_id": order_book_id,
                    "institution_participation": float(group.iloc[i]['institution_participation'])
                    if not pd.isna(group.iloc[i]['institution_participation']) else None,
                    "liquidity_dryup": float(group.iloc[i]['liquidity_dryup'])
                    if not pd.isna(group.iloc[i]['liquidity_dryup']) else None,
                    "consecutive_limit_up": int(group.iloc[i]['consecutive_limit_up']),
                    "daily_return": float(returns[i]) if i < len(returns) and not np.isnan(returns[i]) else None,
                    "dynamic_risk_premium": float(risk_premium[i]) if not np.isnan(risk_premium[i]) else None,
                    "liquidity_impact": float(liquidity_impact[i]) if not np.isnan(liquidity_impact[i]) else None,
                    "tail_risk_probability": float(tail_risk_prob[i]),
                    "emotion_liquidity_match": float(emotion_liquidity_match.iloc[i])
                    if not pd.isna(emotion_liquidity_match.iloc[i]) else None,
                    "risk_alert": risk_alert
                })

        # indicator_explanations = {
        #     "date": "äº¤æ˜“æ—¥æœŸï¼Œä½œä¸ºåŸºç¡€æ—¶é—´æˆ³ç”¨äºè¿½è¸ªè¶‹åŠ¿å˜åŒ–ã€‚",
        #     "order_book_id": "è‚¡ç¥¨ä»£ç ï¼Œç”¨äºæ ‡è¯†åˆ†æçš„å…·ä½“æ ‡çš„å¯¹è±¡ã€‚",
        #     "institution_participation": "æœºæ„å‚ä¸åº¦ï¼Œè®¡ç®—æ–¹å¼ä¸º volume / num_tradesï¼Œè¡¨ç¤ºå¹³å‡æ¯ç¬”äº¤æ˜“çš„è‚¡æ•°ã€‚>2000 è¡¨ç¤ºæœºæ„ä¸»å¯¼ï¼Œè¶‹åŠ¿å¯èƒ½å»¶ç»­ï¼›<1000 è¡¨ç¤ºæ•£æˆ·ä¸»å¯¼ï¼Œæ˜“è¿½æ¶¨æ€è·Œã€æ³¢åŠ¨å¤§ï¼›å€¼åœ¨ä¸­é—´ï¼ˆå¦‚1765.58ï¼‰è¡¨ç¤ºå¤„äºä¸­é—´åæœºæ„çŠ¶æ€ï¼Œä½†å°šæœªå½¢æˆç¨³å®šä¸»åŠ›ã€‚",
        #     "liquidity_dryup": "æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°ï¼Œè®¡ç®—æ–¹å¼ä¸º (limit_up - close)/(high-low) + (close - limit_down)/(high-low)ï¼Œåæ˜ ä»·æ ¼ç¦»æ¶¨è·Œåœçš„æ¥è¿‘ç¨‹åº¦ã€‚æ¥è¿‘0 è¡¨ç¤ºä»·æ ¼å¡åœ¨æ¶¨åœæˆ–è·Œåœï¼ŒæµåŠ¨æ€§æ¯ç«­ï¼Œå­˜åœ¨é£é™©ï¼›æ¥è¿‘2 è¡¨ç¤ºä»·æ ¼åœ¨ä¸­é—´åŒºåŸŸï¼Œäº¤æ˜“æ´»è·ƒï¼›å€¼ä¸º0.752 è¡¨ç¤ºä»·æ ¼åå‘è·Œåœï¼ŒæµåŠ¨æ€§ç´§å¼ ï¼ŒæŠ›å”®å‹åŠ›ä»åœ¨ã€‚",
        #     "consecutive_limit_up": "è¿ç»­æ¶¨åœå¤©æ•°ï¼Œç»Ÿè®¡è¿ç»­æ”¶ç›˜ä»·â‰¥æ¶¨åœä»·çš„å¤©æ•°ï¼Œåæ˜ å½“å‰æ˜¯å¦å¤„äºå¼ºåŠ¿ä¸Šæ¶¨è¶‹åŠ¿ã€‚>3 è¡¨ç¤ºå¼ºè¶‹åŠ¿ï¼Œä½†éœ€è­¦æƒ•æƒ…ç»ªè¿‡çƒ­ï¼›=0 è¡¨ç¤ºæ— è¶‹åŠ¿æˆ–è¶‹åŠ¿ä¸­æ–­ï¼›å€¼ä¸º0 è¡¨ç¤ºè¶‹åŠ¿æœªå¯åŠ¨ï¼Œå¸‚åœºä»å¼±ã€‚",
        #     "daily_return": "æ—¥æ”¶ç›Šç‡ï¼Œè®¡ç®—æ–¹å¼ä¸º close/prev_close - 1ï¼Œè¡¨ç¤ºå½“å¤©çš„æ¶¨è·Œå¹…ã€‚å€¼ä¸º-0.0298 è¡¨ç¤ºä¸‹è·Œ2.98%ï¼Œå±äºæ˜¾è‘—å›è°ƒï¼Œç»“åˆå…¶ä»–æŒ‡æ ‡å¯åˆ¤æ–­ä¸ºææ…Œæ€§æŠ›å”®ã€‚",
        #     "dynamic_risk_premium": "åŠ¨æ€é£é™©æº¢ä»·ï¼Œç”±TVP-SSMæ¨¡å‹ä¼°è®¡å¾—å‡ºï¼Œåæ˜ å¸‚åœºå½“å¤©è¦æ±‚å¤šå°‘é¢å¤–å›æŠ¥æ¥æ‰¿æ‹…é£é™©ã€‚null è¡¨ç¤ºæ¨¡å‹éœ€è¦è‡³å°‘30å¤©æ•°æ®æ‰èƒ½è¾“å‡ºï¼Œå½“å‰ä¸ºé¢„çƒ­é˜¶æ®µï¼›åç»­è‹¥è·³å‡è¶…è¿‡0.5%ï¼Œè¡¨æ˜å¸‚åœºæåº¦ææ…Œï¼Œé£é™©åå¥½ä¸‹é™ã€‚",
        #     "liquidity_impact": "æµåŠ¨æ€§å†²å‡»ç³»æ•°ï¼Œç”±TVP-SSMæ¨¡å‹ä¼°è®¡å¾—å‡ºï¼Œè¡¡é‡æµåŠ¨æ€§æ¶åŒ–å¯¹é£é™©æº¢ä»·çš„æ”¾å¤§ä½œç”¨ã€‚null è¡¨ç¤ºæ¨¡å‹é¢„çƒ­ä¸­ï¼›åç»­è‹¥ä¸ºæ­£ä¸”æŒç»­å¢å¤§ï¼Œè¡¨æ˜æµåŠ¨æ€§å·²æˆä¸ºä¸»è¦é£é™©æ¥æºã€‚",
        #     "tail_risk_probability": "å°¾éƒ¨é£é™©æ¦‚ç‡ï¼ŒåŸºäºå†å²å›æµ‹è§„åˆ™ç”Ÿæˆï¼Œè¡¨ç¤ºæœªæ¥30å¤©å‡ºç°å¤§å¹…å›æ’¤çš„æ¦‚ç‡ã€‚0.0 è¡¨ç¤ºå½“å‰ä¸æ»¡è¶³é«˜é£é™©æ¡ä»¶ï¼›>0.65 è¡¨ç¤ºæé«˜é£é™©ï¼Œå»ºè®®å‡ä»“æˆ–å¯¹å†²ï¼›è§¦å‘æ¡ä»¶ä¸º liquidity_dryup > 0.8 ä¸” dynamic_risk_premium å¤„äºé«˜ä½ã€‚",
        #     "emotion_liquidity_match": "æƒ…ç»ª-æµåŠ¨æ€§åŒ¹é…åº¦ï¼Œè®¡ç®—æ–¹å¼ä¸º (æœºæ„å‚ä¸åº¦ - å¸‚åœºå‡å€¼) Ã— (1 - liquidity_dryup)ï¼Œç”¨äºåˆ¤æ–­â€˜è°åœ¨ä¹°â€™å’Œâ€˜èƒ½ä¸èƒ½å–â€™æ˜¯å¦åè°ƒã€‚>0 è¡¨ç¤ºå¥åº·çŠ¶æ€ï¼ˆæœºæ„ä¹°å…¥ä¸”æµåŠ¨æ€§å¥½ï¼‰ï¼›<-0.3 è¡¨ç¤ºå±é™©ä¿¡å·ï¼ˆæœºæ„æ’¤ç¦»ä¸”æµåŠ¨æ€§å·®ï¼‰ï¼›å€¼ä¸º-0.12 è¡¨ç¤ºè½»åº¦ä¸åŒ¹é…ï¼Œä½†æœªè¾¾åˆ°è­¦æˆ’æ°´å¹³ã€‚",
        #     "risk_alert": "é£é™©é¢„è­¦ï¼Œç”±ç»¼åˆé€»è¾‘åˆ¤æ–­ç”Ÿæˆï¼ŒæŒ‡ç¤ºæ˜¯å¦åº”å‘å‡ºå‡ä»“ä¿¡å·ã€‚False è¡¨ç¤ºæš‚æ— ç³»ç»Ÿæ€§é£é™©ï¼›True è¡¨ç¤ºæ»¡è¶³å¤šé‡é«˜é£é™©æ¡ä»¶ï¼ˆå¦‚æµåŠ¨æ€§æ¯ç«­ã€æƒ…ç»ªæ¶åŒ–ç­‰ï¼‰ï¼Œå¼ºçƒˆå»ºè®®é‡‡å–è¡ŒåŠ¨ã€‚"
        # }
        return results

    def summarize_CSanalysis(self, start_date: int, end_date: int, target_stock_id=None,
                                  order_book_id_list: list = None, lookback_days=30, confidence_level=0.95):
        """
        å¯¹æ·±åº¦åˆ†ææŒ‡æ ‡è¿›è¡Œæ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æï¼Œè¯†åˆ«åŠ¨æ€æ¨¡å¼ä¸é¢†å…ˆ-æ»åå…³ç³»

        è¿”å›:
        str: åŒ…å«æ·±åº¦è¶‹åŠ¿åˆ†æçš„è‡ªç„¶è¯­è¨€æ€»ç»“
        """
        analysis_results = self._analyze_CS(start_date, end_date, order_book_id_list)
        if not analysis_results:
            return "æ— æ•°æ®å¯ä¾›åˆ†æã€‚"

        # è½¬æ¢ä¸ºDataFrameå¹¶é¢„å¤„ç†
        df = pd.DataFrame(analysis_results)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values(['order_book_id', 'date'])

        # ======================
        # æ–°å¢ï¼šå¸‚åœºåŸºå‡†è®¡ç®—ï¼ˆä¿ç•™åŸä»£ç ä¸å˜ï¼Œæ–°å¢æ­¤éƒ¨åˆ†ï¼‰
        # ======================
        # è·å–æœ€æ–°æ—¥æœŸç”¨äºå¸‚åœºåŸºå‡†è®¡ç®—
        latest_date = df['date'].max()

        # è·å–æ‰€æœ‰è‚¡ç¥¨åœ¨æœ€æ–°æ—¥æœŸçš„æ•°æ®
        market_df = df[df['date'] == latest_date].copy()

        # è®¡ç®—å¸‚åœºåŸºå‡†ï¼ˆæ’é™¤ç›®æ ‡è‚¡ç¥¨è‡ªèº«ï¼‰
        market_benchmarks = {}
        if len(market_df) > 1:  # è‡³å°‘æœ‰2åªè‚¡ç¥¨æ‰èƒ½è®¡ç®—åŸºå‡†
            if target_stock_id:
                market_without_target = market_df[market_df['order_book_id'] != target_stock_id]
            else:
                market_without_target = market_df  # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡è‚¡ç¥¨ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®

            if not market_without_target.empty:
                market_benchmarks = {
                    'inst_part_mean': market_without_target['institution_participation'].mean(),
                    'inst_part_25pct': market_without_target['institution_participation'].quantile(0.25),
                    'inst_part_75pct': market_without_target['institution_participation'].quantile(0.75),
                    'liquidity_mean': market_without_target['liquidity_dryup'].mean(),
                    'liquidity_25pct': market_without_target['liquidity_dryup'].quantile(0.25),
                    'liquidity_75pct': market_without_target['liquidity_dryup'].quantile(0.75),
                }
                # è®¡ç®—é£é™©æº¢ä»·åŸºå‡†ï¼ˆä»…åŒ…å«æœ‰æ•ˆå€¼ï¼‰
                valid_rp = market_without_target['dynamic_risk_premium'].dropna()
                if not valid_rp.empty:
                    market_benchmarks['risk_premium_mean'] = valid_rp.mean()

        # é€‰æ‹©ç›®æ ‡è‚¡ç¥¨
        if target_stock_id:
            stock_df = df[df['order_book_id'] == target_stock_id].copy()
            if stock_df.empty:
                return f"æœªæ‰¾åˆ°è‚¡ç¥¨ {target_stock_id} çš„æ•°æ®ã€‚"
        else:
            # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªè‚¡ç¥¨
            target_stock_id = df['order_book_id'].iloc[0]
            stock_df = df[df['order_book_id'] == target_stock_id].copy()

        # é™åˆ¶åˆ†æçª—å£
        if len(stock_df) > lookback_days:
            stock_df = stock_df.tail(lookback_days).reset_index(drop=True)

        n = len(stock_df)
        if n < 10:  # éœ€è¦è¶³å¤Ÿæ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ
            return f"è‚¡ç¥¨ {target_stock_id} æ•°æ®ç‚¹ä¸è¶³ï¼ˆ{n}å¤©ï¼‰ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆè¶‹åŠ¿åˆ†æã€‚"

        # ======================
        # 1. æ·±åº¦æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ
        # ======================

        # --- 1.1 æœºæ„å‚ä¸åº¦è¶‹åŠ¿ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰---
        inst_part = stock_df['institution_participation'].astype(float)

        # è®¡ç®—çº¿æ€§è¶‹åŠ¿æ–œç‡å’Œæ˜¾è‘—æ€§
        x = np.arange(n)
        slope_inst, intercept_inst, r_inst, p_inst, std_err_inst = stats.linregress(x, inst_part)
        trend_strength_inst = abs(slope_inst) * n / inst_part.mean()

        # è¶‹åŠ¿åˆ†ç±»ï¼ˆåŸºäºç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¼ºåº¦ï¼‰
        inst_trend_desc = ""
        if p_inst < (1 - confidence_level):
            if slope_inst > 0:
                if trend_strength_inst > 0.5:
                    inst_trend_desc = "æ˜¾è‘—ä¸Šå‡è¶‹åŠ¿ï¼Œæœºæ„èµ„é‡‘æŒç»­å¤§å¹…æµå…¥"
                elif trend_strength_inst > 0.2:
                    inst_trend_desc = "æ¸©å’Œä¸Šå‡è¶‹åŠ¿ï¼Œæœºæ„èµ„é‡‘é€æ­¥ä»‹å…¥"
                else:
                    inst_trend_desc = "è½»å¾®ä¸Šå‡è¶‹åŠ¿ï¼Œæœºæ„å‚ä¸åº¦ç¼“æ…¢æ”¹å–„"
            else:
                if trend_strength_inst > 0.5:
                    inst_trend_desc = "æ˜¾è‘—ä¸‹é™è¶‹åŠ¿ï¼Œæœºæ„èµ„é‡‘åŠ é€Ÿæ’¤ç¦»"
                elif trend_strength_inst > 0.2:
                    inst_trend_desc = "æ¸©å’Œä¸‹é™è¶‹åŠ¿ï¼Œæœºæ„èµ„é‡‘ç¼“æ…¢æµå‡º"
                else:
                    inst_trend_desc = "è½»å¾®ä¸‹é™è¶‹åŠ¿ï¼Œæœºæ„å‚ä¸åº¦ç¼“æ…¢é™ä½"

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            inst_trend_desc += f"ï¼ˆæ–œç‡={slope_inst:.2f}, p={p_inst:.3f}ï¼‰"
        else:
            inst_trend_desc = "æ— æ˜¾è‘—è¶‹åŠ¿ï¼Œæœºæ„å‚ä¸åº¦éšæœºæ³¢åŠ¨"

        # --- 1.2 æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°è¶‹åŠ¿ ---
        liquidity = stock_df['liquidity_dryup'].astype(float)

        # æ£€æµ‹åŠ é€Ÿæ¶åŒ–ï¼ˆäºŒé˜¶å¯¼æ•°è¿‘ä¼¼ï¼‰
        liquidity_ma = liquidity.rolling(window=5).mean().dropna()
        liquidity_accel = liquidity_ma.diff().mean()

        # è¶‹åŠ¿åˆ†ç±»
        liquidity_status = ""
        if liquidity_accel > 0.03 and liquidity.iloc[-1] > 0.7:
            liquidity_status = f"æµåŠ¨æ€§æ­£åœ¨åŠ é€Ÿæ¶åŒ–ï¼ˆåŠ é€Ÿåº¦={liquidity_accel:.4f}ï¼‰ï¼Œä»·æ ¼æŒç»­è´´è¿‘è·Œåœï¼Œäº¤æ˜“æåº¦å›°éš¾"
        elif liquidity.iloc[-1] > 0.8:
            liquidity_status = "æµåŠ¨æ€§ä¸¥é‡ç´§å¼ ï¼Œä»·æ ¼é¢‘ç¹è§¦åŠæ¶¨è·Œåœï¼Œäº¤æ˜“å›°éš¾"
        elif liquidity.iloc[-1] > 0.6:
            liquidity_status = "æµåŠ¨æ€§ç´§å¼ ï¼Œä»·æ ¼æ¥è¿‘æ¶¨è·ŒåœåŒºé—´"
        elif liquidity.iloc[-1] < 0.3:
            liquidity_status = "æµåŠ¨æ€§å……è¶³ï¼Œä»·æ ¼è¿è¡Œå¹³ç¨³ï¼Œäº¤æ˜“æ´»è·ƒ"
        else:
            liquidity_status = "æµåŠ¨æ€§å¤„äºæ­£å¸¸æ°´å¹³"

        # --- 1.3 é£é™©æº¢ä»·åŠ¨æ€åˆ†æï¼ˆæ ¸å¿ƒè¶‹åŠ¿ï¼‰---
        risk_premium = stock_df['dynamic_risk_premium'].dropna()
        if len(risk_premium) >= 15:
            x_rp = np.arange(len(risk_premium))
            slope_rp, _, _, p_rp, _ = stats.linregress(x_rp, risk_premium)

            # è®¡ç®—æ³¢åŠ¨ç‡å˜åŒ–
            rp_vol = risk_premium.diff().abs().rolling(window=5).mean().dropna()
            vol_trend = "æ˜¾è‘—ä¸Šå‡" if rp_vol.iloc[-1] > rp_vol.quantile(0.75) else "è¶‹äºç¨³å®š"

            if p_rp < (1 - confidence_level) and slope_rp > 0:
                risk_summary = f"åŠ¨æ€é£é™©æº¢ä»·å‘ˆæ˜¾è‘—ä¸Šå‡è¶‹åŠ¿ï¼ˆæ–œç‡={slope_rp:.4f}, p={p_rp:.3f}ï¼‰ï¼Œä¸”æ³¢åŠ¨ç‡{vol_trend}ï¼Œæ˜¾ç¤ºå¸‚åœºé¿é™©æƒ…ç»ªå¿«é€Ÿå‡æ¸©"
            elif p_rp < (1 - confidence_level) and slope_rp < 0:
                risk_summary = f"åŠ¨æ€é£é™©æº¢ä»·å‘ˆæ˜¾è‘—ä¸‹é™è¶‹åŠ¿ï¼ˆæ–œç‡={slope_rp:.4f}, p={p_rp:.3f}ï¼‰ï¼Œä¸”æ³¢åŠ¨ç‡{vol_trend}ï¼Œæ˜¾ç¤ºå¸‚åœºé£é™©åå¥½å›å‡"
            else:
                risk_summary = f"åŠ¨æ€é£é™©æº¢ä»·æ³¢åŠ¨è¾ƒå¤§ä½†æ— æ˜¾è‘—è¶‹åŠ¿ï¼ˆp={p_rp:.3f}ï¼‰ï¼Œæ³¢åŠ¨ç‡{vol_trend}ï¼Œå¸‚åœºæƒ…ç»ªå¤„äºå¹³è¡¡çŠ¶æ€"
        else:
            risk_summary = "åŠ¨æ€é£é™©æº¢ä»·æ•°æ®ä¸è¶³ï¼Œæš‚æ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ"

        # --- 1.4 å°¾éƒ¨é£é™©åŠ¨æ€æ¨¡å¼ ---
        tail_risk = stock_df['tail_risk_probability'].astype(float)
        high_risk_days = (tail_risk >= 0.65).sum()
        medium_risk_days = ((tail_risk >= 0.4) & (tail_risk < 0.65)).sum()
        risk_persistence = (tail_risk > 0).astype(int).diff().ne(1).cumsum().value_counts().max() / n

        if high_risk_days > n * 0.2:
            tail_risk_summary = f"é«˜é£é™©çŠ¶æ€é¢‘ç¹ï¼ˆ{high_risk_days}å¤©ï¼Œå æ¯”{high_risk_days / n:.0%}ï¼‰ï¼Œä¸”æŒç»­æ€§å¼ºï¼ˆå¹³å‡æŒç»­{int(1 / risk_persistence)}å¤©ï¼‰ï¼Œç³»ç»Ÿæ€§é£é™©ç´¯ç§¯æ˜æ˜¾"
        elif high_risk_days > 0:
            tail_risk_summary = f"å¶å‘é«˜é£é™©çŠ¶æ€ï¼ˆ{high_risk_days}å¤©ï¼‰ï¼Œä½†æŒç»­æ—¶é—´çŸ­ï¼Œéœ€è­¦æƒ•çªå‘é£é™©"
        elif medium_risk_days > n * 0.3:
            tail_risk_summary = f"ä¸­ç­‰é£é™©çŠ¶æ€æŒç»­ï¼ˆ{medium_risk_days}å¤©ï¼Œå æ¯”{medium_risk_days / n:.0%}ï¼‰ï¼Œå¸‚åœºè„†å¼±æ€§å¢å¼º"
        else:
            tail_risk_summary = "é£é™©æ°´å¹³æ•´ä½“å¯æ§ï¼Œç³»ç»Ÿæ€§å´©æºƒæ¦‚ç‡ä½"

        # --- 1.5 é£é™©é¢„è­¦ä¿¡å·æ¨¡å¼åˆ†æ ---
        risk_alerts = stock_df['risk_alert'].astype(bool)
        alert_count = risk_alerts.sum()
        alert_clusters = (risk_alerts != risk_alerts.shift()).cumsum()[risk_alerts].value_counts()
        avg_cluster_size = alert_clusters.mean() if not alert_clusters.empty else 0

        if alert_count > n * 0.2:
            alert_summary = f"é£é™©é¢„è­¦é«˜é¢‘è§¦å‘ï¼ˆ{alert_count}æ¬¡ï¼Œ{alert_count / n:.0%}å¤©ï¼‰ï¼Œä¸”å¸¸æˆç°‡å‡ºç°ï¼ˆå¹³å‡{avg_cluster_size:.1f}å¤©/ç°‡ï¼‰ï¼Œå¸‚åœºå¤„äºæŒç»­å±é™©çŠ¶æ€"
        elif alert_count > 0:
            alert_summary = f"å¶å‘é£é™©é¢„è­¦ï¼ˆ{alert_count}æ¬¡ï¼‰ï¼Œå¤šä¸ºå­¤ç«‹äº‹ä»¶ï¼Œä½†éœ€å…³æ³¨è§¦å‘æ¡ä»¶"
        else:
            alert_summary = "æœªè§¦å‘é£é™©é¢„è­¦ï¼Œå½“å‰å¸‚åœºç¯å¢ƒç›¸å¯¹å®‰å…¨"

        # --- 1.6 æƒ…ç»ª-æµåŠ¨æ€§åŠ¨æ€å…³ç³» ---
        match_series = stock_df['emotion_liquidity_match'].astype(float)

        # è®¡ç®—ä¸æœºæ„å‚ä¸åº¦çš„æ»šåŠ¨ç›¸å…³æ€§
        rolling_corr = inst_part.rolling(window=5).corr(match_series)
        avg_corr = rolling_corr.mean()

        if match_series.mean() < -0.25:
            match_trend = f"æŒç»­ä¸¥é‡è´Ÿå‘ï¼ˆå‡å€¼={match_series.mean():.2f}ï¼‰ï¼Œæœºæ„æ’¤ç¦»ä¸æµåŠ¨æ€§æ¶åŒ–å½¢æˆæ¶æ€§å¾ªç¯"
        elif match_series.mean() < -0.1:
            match_trend = f"æŒç»­è½»åº¦è´Ÿå‘ï¼ˆå‡å€¼={match_series.mean():.2f}ï¼‰ï¼Œéœ€å…³æ³¨èµ„é‡‘æµå‘å˜åŒ–"
        elif match_series.mean() > 0.25:
            match_trend = f"æŒç»­é«˜åº¦åè°ƒï¼ˆå‡å€¼={match_series.mean():.2f}ï¼‰ï¼Œæœºæ„ä¸»å¯¼ä¸”æµåŠ¨æ€§å¥½ï¼Œè¶‹åŠ¿å¥åº·"
        else:
            match_trend = f"åŸºæœ¬å¹³è¡¡ï¼ˆå‡å€¼={match_series.mean():.2f}ï¼‰ï¼Œå¸‚åœºå¤„äºè¿‡æ¸¡çŠ¶æ€"

        # --- 1.7 é¢†å…ˆ-æ»åå…³ç³»åˆ†æï¼ˆå…³é”®ï¼ï¼‰---
        # æ£€æŸ¥æµåŠ¨æ€§æ¯ç«­æ˜¯å¦é¢†å…ˆäºé£é™©æº¢ä»·å˜åŒ–
        lead_lag_results = []
        best_lag = None
        if len(risk_premium) >= 20:
            for lag in range(-7, 8):  # -7åˆ°+7å¤©çš„æ»å
                if lag <= 0:
                    corr = liquidity[:lag].corr(risk_premium[-lag:]) if lag != 0 else liquidity.corr(risk_premium)
                else:
                    corr = liquidity[lag:].corr(risk_premium[:-lag])
                lead_lag_results.append((lag, corr))

            best_lag, best_corr = max(lead_lag_results, key=lambda x: abs(x[1]))
            if abs(best_corr) > 0.45:
                if best_lag < 0:
                    lead_lag_summary = f"æµåŠ¨æ€§æ¯ç«­é¢†å…ˆé£é™©æº¢ä»·çº¦{-best_lag}å¤©ï¼ˆæœ€å¤§ç›¸å…³ç³»æ•°={best_corr:.2f}ï¼‰ï¼Œæ˜¯å¸‚åœºé£é™©çš„å…ˆè¡ŒæŒ‡æ ‡"
                elif best_lag > 0:
                    lead_lag_summary = f"é£é™©æº¢ä»·é¢†å…ˆæµåŠ¨æ€§æ¯ç«­çº¦{best_lag}å¤©ï¼ˆæœ€å¤§ç›¸å…³ç³»æ•°={best_corr:.2f}ï¼‰ï¼Œé£é™©æƒ…ç»ªå…ˆäºæµåŠ¨æ€§å˜åŒ–"
                else:
                    lead_lag_summary = f"æµåŠ¨æ€§æ¯ç«­ä¸é£é™©æº¢ä»·åŒæ­¥å˜åŒ–ï¼ˆç›¸å…³ç³»æ•°={best_corr:.2f}ï¼‰ï¼Œé£é™©ä¸æµåŠ¨æ€§ç›¸äº’å¼ºåŒ–"
            else:
                lead_lag_summary = "æµåŠ¨æ€§ä¸é£é™©æº¢ä»·å…³ç³»ä¸ç¨³å®šï¼Œæ— æ˜æ˜¾é¢†å…ˆ-æ»åæ¨¡å¼"
        else:
            lead_lag_summary = "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢†å…ˆ-æ»ååˆ†æ"

        # ======================
        # 2. è¯†åˆ«å…³é”®åŠ¨æ€æ¨¡å¼
        # ======================

        # æ¨¡å¼1: æµåŠ¨æ€§å±æœºæ¨¡å¼
        liquidity_crisis = (
            "æµåŠ¨æ€§åŠ é€Ÿæ¶åŒ–" in liquidity_status and
            "æ˜¾è‘—ä¸‹é™è¶‹åŠ¿" in inst_trend_desc and
            "ä¸Šå‡è¶‹åŠ¿" in risk_summary and
            best_lag < 0 if 'best_lag' in locals() else False
        )

        # æ¨¡å¼2: å¥åº·ä¸Šæ¶¨æ¨¡å¼
        healthy_rally = (
                "æ˜¾è‘—ä¸Šå‡è¶‹åŠ¿" in inst_trend_desc and
                "æµåŠ¨æ€§å……è¶³" in liquidity_status and
                ("ä¸‹é™è¶‹åŠ¿" in risk_summary or "å¹³è¡¡" in risk_summary)
        )

        # æ¨¡å¼3: æƒ…ç»ªé©±åŠ¨æ³¢åŠ¨æ¨¡å¼
        emotion_volatility = (
                abs(avg_corr) < 0.2 and
                "æ³¢åŠ¨å‰§çƒˆ" in risk_summary and
                "åŸºæœ¬å¹³è¡¡" in match_trend
        )

        # ======================
        # 3. æ–°å¢ï¼šç›¸å¯¹å¸‚åœºå®šä½åˆ†æï¼ˆä¿ç•™åŸä»£ç ä¸å˜ï¼Œæ–°å¢æ­¤éƒ¨åˆ†ï¼‰
        # ======================
        relative_analysis = ""
        latest = stock_df.iloc[-1]

        # 1. æœºæ„å‚ä¸åº¦ç›¸å¯¹ä½ç½®
        inst_relative_desc = "æ— å¸‚åœºæ¯”è¾ƒæ•°æ®"
        inst_part_relative = None
        if 'inst_part_mean' in market_benchmarks:
            # è®¡ç®—Z-scoreï¼ˆä½¿ç”¨IQRæ ‡å‡†åŒ–ï¼‰
            iqr = market_benchmarks['inst_part_75pct'] - market_benchmarks['inst_part_25pct']
            if iqr > 0:  # é˜²æ­¢é™¤ä»¥0
                inst_part_relative = (
                        (latest['institution_participation'] - market_benchmarks['inst_part_mean']) /
                        (iqr + 1e-5)
                )
                if inst_part_relative > 1.0:
                    inst_relative_desc = "æ˜¾è‘—é«˜äºå¸‚åœºå¹³å‡æ°´å¹³ï¼Œæœºæ„å…³æ³¨åº¦çªå‡º"
                elif inst_part_relative > 0.5:
                    inst_relative_desc = "é«˜äºå¸‚åœºå¹³å‡æ°´å¹³ï¼Œæœºæ„å…³æ³¨åº¦è¾ƒé«˜"
                elif inst_part_relative < -1.0:
                    inst_relative_desc = "æ˜¾è‘—ä½äºå¸‚åœºå¹³å‡æ°´å¹³ï¼Œæœºæ„å…³æ³¨åº¦ä½è¿·"
                elif inst_part_relative < -0.5:
                    inst_relative_desc = "ä½äºå¸‚åœºå¹³å‡æ°´å¹³ï¼Œæœºæ„å…³æ³¨åº¦è¾ƒä½"
                else:
                    inst_relative_desc = "æ¥è¿‘å¸‚åœºå¹³å‡æ°´å¹³"

        # 2. æµåŠ¨æ€§ç›¸å¯¹ä½ç½®
        liquidity_relative_desc = "æ— å¸‚åœºæ¯”è¾ƒæ•°æ®"
        liquidity_relative = None
        if 'liquidity_mean' in market_benchmarks:
            # è®¡ç®—Z-scoreï¼ˆä½¿ç”¨IQRæ ‡å‡†åŒ–ï¼‰
            iqr = market_benchmarks['liquidity_75pct'] - market_benchmarks['liquidity_25pct']
            if iqr > 0:  # é˜²æ­¢é™¤ä»¥0
                liquidity_relative = (
                        (latest['liquidity_dryup'] - market_benchmarks['liquidity_mean']) /
                        (iqr + 1e-5)
                )
                if liquidity_relative > 1.0:
                    liquidity_relative_desc = "æµåŠ¨æ€§ç´§å¼ ç¨‹åº¦æ˜¾è‘—é«˜äºå¸‚åœºï¼Œäº¤æ˜“éš¾åº¦å¤§"
                elif liquidity_relative > 0.5:
                    liquidity_relative_desc = "æµåŠ¨æ€§ç´§å¼ ç¨‹åº¦é«˜äºå¸‚åœº"
                elif liquidity_relative < -1.0:
                    liquidity_relative_desc = "æµåŠ¨æ€§æ˜¾è‘—ä¼˜äºå¸‚åœºï¼Œäº¤æ˜“é¡ºç•…"
                elif liquidity_relative < -0.5:
                    liquidity_relative_desc = "æµåŠ¨æ€§ä¼˜äºå¸‚åœº"
                else:
                    liquidity_relative_desc = "æµåŠ¨æ€§å¤„äºå¸‚åœºæ­£å¸¸æ°´å¹³"

        # 3. é£é™©æº¢ä»·ç›¸å¯¹ä½ç½®
        risk_relative_desc = "æ— å¸‚åœºæ¯”è¾ƒæ•°æ®æˆ–æ•°æ®ä¸è¶³"
        if 'risk_premium_mean' in market_benchmarks and not pd.isna(latest['dynamic_risk_premium']):
            risk_premium_relative = (
                    latest['dynamic_risk_premium'] - market_benchmarks['risk_premium_mean']
            )
            if risk_premium_relative > 0.003:
                risk_relative_desc = "é£é™©æº¢ä»·æ˜¾è‘—é«˜äºå¸‚åœºï¼Œé¿é™©æƒ…ç»ªå¼ºçƒˆ"
            elif risk_premium_relative > 0.001:
                risk_relative_desc = "é£é™©æº¢ä»·é«˜äºå¸‚åœºï¼Œé¿é™©æƒ…ç»ªè¾ƒé«˜"
            elif risk_premium_relative < -0.003:
                risk_relative_desc = "é£é™©æº¢ä»·æ˜¾è‘—ä½äºå¸‚åœºï¼Œé£é™©åå¥½çªå‡º"
            elif risk_premium_relative < -0.001:
                risk_relative_desc = "é£é™©æº¢ä»·ä½äºå¸‚åœºï¼Œé£é™©åå¥½è¾ƒé«˜"
            else:
                risk_relative_desc = "é£é™©æº¢ä»·æ¥è¿‘å¸‚åœºæ°´å¹³"

        # ======================
        # 4. ç»¼åˆæ€»ç»“è¾“å‡ºï¼ˆä¿®æ”¹è¿™éƒ¨åˆ†ä»¥åŒ…å«ç›¸å¯¹åˆ†æï¼‰
        # ======================
        summary = f"""
        ã€{target_stock_id} æ·±åº¦è¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‘ï¼ˆæˆªè‡³ {stock_df['date'].max().strftime('%Y-%m-%d')}ï¼‰
    
        ğŸŒ å¸‚åœºç›¸å¯¹å®šä½ï¼ˆåŸºäº{len(market_df)}åªè‚¡ç¥¨æœ€æ–°æ•°æ®ï¼‰ï¼š
        - æœºæ„å‚ä¸åº¦ï¼š{inst_relative_desc}
        - æµåŠ¨æ€§çŠ¶å†µï¼š{liquidity_relative_desc}
        - é£é™©æº¢ä»·æ°´å¹³ï¼š{risk_relative_desc}
    
        ğŸ” æ ¸å¿ƒè¶‹åŠ¿è¯Šæ–­ï¼ˆåŸºäº{len(stock_df)}å¤©æ•°æ®ï¼‰ï¼š
        1. **æœºæ„å‚ä¸è¶‹åŠ¿**ï¼š{inst_trend_desc}
           - å½“å‰å€¼ï¼š{inst_part.iloc[-1]:.2f}ï¼ˆ{('â†‘' if slope_inst > 0 else 'â†“') if p_inst < 0.05 else 'â†’'}ï¼‰
           - ç›¸å¯¹å¸‚åœºä½ç½®ï¼š{'é«˜äº' if inst_part_relative and inst_part_relative > 0 else 'ä½äº' if inst_part_relative and inst_part_relative < 0 else 'æ¥è¿‘'}å¸‚åœºä¸­ä½æ•°
           - 5æ—¥ç§»åŠ¨å¹³å‡ï¼š{inst_part.rolling(5).mean().iloc[-1]:.2f}
    
        2. **æµåŠ¨æ€§çŠ¶å†µ**ï¼š{liquidity_status}
           - å½“å‰æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°ï¼š{liquidity.iloc[-1]:.3f}
           - æµåŠ¨æ€§åŠ é€Ÿåº¦ï¼š{liquidity_accel:.4f}ï¼ˆæ­£å€¼è¡¨ç¤ºæ¶åŒ–åŠ é€Ÿï¼‰
           - ç›¸å¯¹å¸‚åœºä½ç½®ï¼š{'ç´§å¼ ç¨‹åº¦é«˜äº' if liquidity_relative and liquidity_relative > 0 else 'ç´§å¼ ç¨‹åº¦ä½äº' if liquidity_relative and liquidity_relative < 0 else 'æ¥è¿‘'}å¸‚åœºå¹³å‡æ°´å¹³
    
        3. **é£é™©æƒ…ç»ªåŠ¨æ€**ï¼š{risk_summary}
           - é£é™©æ³¢åŠ¨ç‡è¶‹åŠ¿ï¼š{vol_trend if 'vol_trend' in locals() else 'N/A'}
           - ç›¸å¯¹å¸‚åœºé£é™©ï¼š{risk_relative_desc.lower()}
    
        4. **å…³é”®åŠ¨æ€å…³ç³»**ï¼š{lead_lag_summary}
           - {'æµåŠ¨æ€§æŒ‡æ ‡å¯ä½œä¸ºé£é™©å˜åŒ–çš„é¢†å…ˆæŒ‡æ ‡ï¼Œæå‰é¢„è­¦å¸‚åœºå‹åŠ›'
            if 'é¢†å…ˆ' in lead_lag_summary and best_lag and best_lag < 0
            else 'é£é™©æƒ…ç»ªå˜åŒ–å…ˆäºæµåŠ¨æ€§æ¶åŒ–ï¼Œéœ€ä¼˜å…ˆå…³æ³¨æƒ…ç»ªæŒ‡æ ‡'
            if 'é¢†å…ˆ' in lead_lag_summary and best_lag and best_lag > 0
            else 'æµåŠ¨æ€§ä¸é£é™©æƒ…ç»ªåŒæ­¥å˜åŒ–ï¼Œéœ€åŒæ—¶ç›‘æ§'}
    
        ğŸ’¡ è¯†åˆ«åˆ°çš„å¸‚åœºæ¨¡å¼ï¼š
        {'âš ï¸ã€æµåŠ¨æ€§å±æœºæ¨¡å¼ã€‘æœºæ„æ’¤ç¦»ã€æµåŠ¨æ€§æ¶åŒ–åŠ é€Ÿä¸”é¢†å…ˆäºé£é™©ä¸Šå‡ï¼Œå¸‚åœºè„†å¼±æ€§æé«˜ï¼' if liquidity_crisis else
            'ğŸ“ˆã€å¥åº·ä¸Šæ¶¨æ¨¡å¼ã€‘æœºæ„æŒç»­æµå…¥ã€æµåŠ¨æ€§å……è¶³ä¸”é£é™©æƒ…ç»ªç¨³å®šï¼Œè¶‹åŠ¿å¥åº·å¯æŒç»­ã€‚' if healthy_rally else
            'ğŸ”„ã€æƒ…ç»ªé©±åŠ¨æ³¢åŠ¨ã€‘å¸‚åœºæƒ…ç»ªä¸æµåŠ¨æ€§åŒ¹é…åº¦ä½ï¼Œä»·æ ¼æ³¢åŠ¨ä¸»è¦ç”±æƒ…ç»ªé©±åŠ¨ï¼Œè¶‹åŠ¿éš¾ä»¥æŒç»­ã€‚' if emotion_volatility else
            'ğŸ”ã€æ··åˆçŠ¶æ€ã€‘å¸‚åœºå¤„äºè¿‡æ¸¡æœŸï¼Œéœ€å¯†åˆ‡å…³æ³¨é¢†å…ˆæŒ‡æ ‡å˜åŒ–æ–¹å‘ã€‚'}
    
        ğŸ“Š é£é™©çŠ¶æ€è¯„ä¼°ï¼š
        - å°¾éƒ¨é£é™©ï¼š{tail_risk_summary}
        - é£é™©é¢„è­¦ï¼š{alert_summary}
        - æƒ…ç»ª-æµåŠ¨æ€§åŒ¹é…ï¼š{match_trend}
    
        ğŸ¯ æ“ä½œå»ºè®®ï¼ˆåŸºäºå½“å‰æ¨¡å¼å’Œå¸‚åœºç›¸å¯¹ä½ç½®ï¼‰ï¼š
        {('ğŸ”´ã€ç´§æ€¥è¡ŒåŠ¨ã€‘æµåŠ¨æ€§å±æœºæ¨¡å¼å·²ç¡®è®¤ï¼å»ºè®®ï¼š' +
        '   - ç«‹å³å‡ä»“50%ä»¥ä¸Šï¼Œä¿ç•™ç°é‡‘åº”å¯¹æµåŠ¨æ€§æ¯ç«­' +
        '   - ä¹°å…¥çŸ­æœŸè™šå€¼PutæœŸæƒå¯¹å†²å°¾éƒ¨é£é™©' +
        '   - å¯†åˆ‡ç›‘æ§æµåŠ¨æ€§æŒ‡æ ‡ï¼Œè‹¥åŠ é€Ÿåº¦ç»§ç»­ä¸Šå‡åˆ™å…¨éƒ¨ç¦»åœº' if liquidity_crisis else
        'ğŸŸ¢ã€ç§¯æå¸ƒå±€ã€‘å¥åº·ä¸Šæ¶¨æ¨¡å¼ç¡®è®¤ï¼å»ºè®®ï¼š' +
        '   - ä¿æŒæˆ–é€‚åº¦åŠ ä»“ï¼Œç›®æ ‡ä»“ä½70-90%' +
        '   - ä½¿ç”¨å¤‡å…‘ç­–ç•¥(Covered Call)å¢å¼ºæ”¶ç›Š' +
        '   - è‹¥æœºæ„å‚ä¸åº¦å¢é€Ÿæ”¾ç¼“åˆ™éƒ¨åˆ†æ­¢ç›ˆ' if healthy_rally else
        'ğŸŸ¡ã€è°¨æ…æ“ä½œã€‘æƒ…ç»ªé©±åŠ¨æ³¢åŠ¨æ¨¡å¼ï¼å»ºè®®ï¼š' +
        '   - é™ä½ä»“ä½è‡³30-50%ï¼Œé¿å…è¿½é«˜æ€è·Œ' +
        '   - é‡‡ç”¨è·¨å¼ç»„åˆ(Straddle)æ•æ‰æ³¢åŠ¨' +
        '   - é‡ç‚¹ç›‘æ§æœºæ„å‚ä¸åº¦å˜åŒ–ï¼Œåˆ¤æ–­è¶‹åŠ¿æ–¹å‘' if emotion_volatility else
        'ğŸ”µã€è§‚å¯Ÿç­‰å¾…ã€‘æ··åˆçŠ¶æ€ï¼å»ºè®®ï¼š' +
        '   - ç»´æŒä¸­æ€§ä»“ä½(40-60%)' +
        '   - è®¾ç½®çªç ´ç­–ç•¥ï¼šè‹¥æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°çªç ´0.85åˆ™å‡ä»“ï¼Œè·Œç ´0.5åˆ™åŠ ä»“' +
        '   - æ¯å‘¨é‡æ–°è¯„ä¼°å¸‚åœºæ¨¡å¼')}
        
        ğŸ“Œ é£é™©æç¤ºï¼š
        - æœ¬åˆ†æåŸºäºå†å²æ•°æ®ï¼Œæœªæ¥å¸‚åœºç»“æ„å¯èƒ½å˜åŒ–
        - å»ºè®®æ¯å‘¨æ›´æ–°åˆ†æï¼Œå°¤å…¶å…³æ³¨é¢†å…ˆæŒ‡æ ‡å˜åŒ–
        
        ğŸ” æ·±åº¦æ´å¯Ÿï¼š
        {('æµåŠ¨æ€§æ¯ç«­æŒ‡æ•°é¢†å…ˆé£é™©æº¢ä»·çº¦' + str(-best_lag) + 'å¤©ï¼Œå¯ä½œä¸ºæ—©æœŸé¢„è­¦ä¿¡å·ã€‚'
        if 'é¢†å…ˆ' in lead_lag_summary and best_lag and best_lag < 0
        else 'é£é™©æƒ…ç»ªå˜åŒ–æ˜¯æµåŠ¨æ€§æ¶åŒ–çš„æ—©æœŸæŒ‡æ ‡ï¼Œæå‰å…³æ³¨é£é™©æº¢ä»·èµ°åŠ¿ã€‚'
        if 'é¢†å…ˆ' in lead_lag_summary and best_lag and best_lag > 0
        else 'æµåŠ¨æ€§ä¸é£é™©æƒ…ç»ªåŒæ­¥å˜åŒ–ï¼Œéœ€åŒæ—¶ç›‘æ§ä¸¤ç±»æŒ‡æ ‡ã€‚')}
        
        ğŸ’¡ ç‰¹åˆ«æç¤ºï¼š
        è¯¥è‚¡ç¥¨å½“å‰è¡¨ç°{('æ˜¾è‘—é¢†å…ˆ' if inst_part_relative and inst_part_relative > 0.5 and liquidity_relative and liquidity_relative < 0 else
        'é¢†å…ˆ' if inst_part_relative and inst_part_relative > 0.3 and liquidity_relative and liquidity_relative < 0.3 else
        'è½åäº' if inst_part_relative and inst_part_relative < -0.5 and liquidity_relative and liquidity_relative > 0.5 else
        'ä¸')}å¸‚åœºæ•´ä½“ï¼Œ{('å»ºè®®' if inst_part_relative and inst_part_relative > 0.3 and liquidity_relative and liquidity_relative < 0.3 else 'è°¨æ…')}{'å¢æŒ' if inst_part_relative and inst_part_relative > 0.5 and liquidity_relative and liquidity_relative < 0 else 'å‡æŒ' if inst_part_relative and inst_part_relative < -0.5 and liquidity_relative and liquidity_relative > 0.5 else 'æŒæœ‰'}
        """.strip()

        return summary

    def construct_contract_features(
            self,
            contract_type: str,
            order_book_id: [str],
            start_date: str,
            end_date: str,
    ) -> str:
        """
        æ„å»ºé€‚ç”¨äºå¤šç§åˆçº¦ç±»å‹çš„å…¨é¢ç‰¹å¾é›†ï¼Œä¸æ¶‰åŠèšåˆæ“ä½œ
        :param order_book_id: ç”¨æˆ·æŒ‡å®šçš„åˆçº¦ä»£ç åˆ—è¡¨ï¼Œä»…å¯¹æ­¤éƒ¨åˆ†æ ·æœ¬å¼€å±•ç‰¹å¾å·¥ç¨‹
        :param contract_type: åˆçº¦ç±»å‹ ('CS', 'ETF', 'INDX', 'Future', 'Option')
        :param start_date: æ•°æ®çš„èµ·å§‹æ—¥æœŸ
        :param end_date: æ•°æ®çš„ç»ˆæ­¢æ—¥æœŸ
        :return: åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrameçš„å­˜å‚¨åœ°å€
        """
        df_addr, df_fields = self.ricequant_service.instruments_features_fetching(contract_type, int(start_date), int(end_date))
        df = pd.read_csv(df_addr)
        order_book_id_str = None
        if order_book_id:
            order_book_id_str = ','.join(sorted(order_book_id))
        order_book_id_hash = hashlib.md5(order_book_id_str.encode('utf-8')).hexdigest()[:10]
        output_path = os.path.join(self.features_data_path, f"{start_date}_{end_date}_{order_book_id_hash}_{contract_type}_features_data.csv")
        if os.path.exists(output_path):
            print("ç‰¹å¾æ–‡ä»¶å·²å­˜åœ¨ï¼")
            return output_path
        else:
            print("ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆ")

        # 1. åŸºç¡€æ•°æ®éªŒè¯å¹¶é€‰æ‹©åˆé€‚çš„æ ·æœ¬&æŒ‰æ—¶é—´æ’åº
        if df.empty:
            raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
        if order_book_id and 'order_book_id' in df.columns:     # ç­›é€‰å‡ºorder_book_idåœ¨ç»™å®šåˆ—è¡¨ä¸­çš„è¡Œ
            df = df[df['order_book_id'].isin(order_book_id)]
        if 'date' in df.columns:
            df = df.sort_values(['date', 'order_book_id'])   # æ•´ä½“æ•°æ®ä¼˜å…ˆã€æŒ‰ç…§æ—¶é—´æ’åºã€‘

        # 2. æ ‡å‡†åŒ–åˆ—åï¼ˆå¤„ç†å¯èƒ½çš„å¤§å°å†™å·®å¼‚ï¼‰
        df = df.copy()
        df.columns = [col.lower() for col in df.columns]

        # 3. æŒ‰åˆçº¦ç±»å‹æ„é€ ç‰¹å¾
        if contract_type not in ['CS', 'ETF', 'INDX', 'Future', 'Option']:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆçº¦ç±»å‹: {contract_type}. å¿…é¡»æ˜¯ CS, ETF, INDX, Future, Option")

        # 4. åˆå§‹åŒ–ç‰¹å¾DataFrame
        features = pd.DataFrame(index=df.index)
        features['date'] = df['date']
        features['order_book_id'] = df['order_book_id']
        features['close'] = df['close']

        # å…³é”®æ­¥éª¤ï¼šåˆ›å»ºåˆ†ç»„å¯¹è±¡
        grouped = df.groupby('order_book_id')

        """ ===== å…±äº«åŸºç¡€ç‰¹å¾ (æ‰€æœ‰åˆçº¦ç±»å‹) ===== """
        # ä»·æ ¼ç‰¹å¾
        features['returns'] = grouped['close'].transform(lambda x: x.pct_change())
        features['log_returns'] = grouped['close'].transform(lambda x: np.log(x / x.shift(1)))

        df['returns'] = features['returns']     # æ— éœ€é‡æ–°åˆ›å»º groupedï¼Œå› ä¸º df å·²ç»æ›´æ–°ï¼Œgrouped ä¼šåœ¨è®¿é—®æ—¶ä½¿ç”¨ df çš„æœ€æ–°åˆ—
        df['log_returns'] = features['log_returns']

        # æ³¢åŠ¨ç‡ç‰¹å¾
        features['vol_10d'] = grouped['returns'].transform(lambda x: x.rolling(10).std()) * np.sqrt(252)
        features['vol_20d'] = grouped['returns'].transform(lambda x: x.rolling(20).std()) * np.sqrt(252)
        features['vol_60d'] = grouped['returns'].transform(lambda x: x.rolling(60).std()) * np.sqrt(252)
        features['vol_ratio_20_60'] = features['vol_20d'] / features['vol_60d']  # æ³¢åŠ¨ç‡æ–œç‡

        # è¶‹åŠ¿ç‰¹å¾
        features['ma_5d'] = grouped['close'].transform(lambda x: x / x.rolling(5).mean() - 1)
        features['ma_20d'] = grouped['close'].transform(lambda x: x / x.rolling(20).mean() - 1)
        features['ma_60d'] = grouped['close'].transform(lambda x: x / x.rolling(60).mean() - 1)

        # åŠ¨é‡ç‰¹å¾
        df['ma_20d'] = features['ma_20d']
        features['ma_momentum'] = grouped['ma_20d'].transform(lambda x: x - x.shift(5))

        # çœŸå®æ³¢å¹…ç‰¹å¾
        if 'high' in df.columns and 'low' in df.columns and 'prev_close' in df.columns:
            # çœŸå®æ³¢å¹…è®¡ç®—
            def calculate_true_range(group):
                prev_close_shifted = group['prev_close'].shift(1)
                true_range_val = np.maximum(
                    group['high'] - group['low'],
                    np.maximum(
                        abs(group['high'] - prev_close_shifted),
                        abs(group['low'] - prev_close_shifted)
                    )
                )
                # ä½¿ç”¨å‰ä¸€æ—¥æ”¶ç›˜ä»·è®¡ç®—ç™¾åˆ†æ¯”TRï¼Œæ³¨æ„åˆ†æ¯ä¹Ÿéœ€è¦ shift(1)
                return true_range_val / group['prev_close'].shift(1)

            features['true_range'] = grouped.apply(calculate_true_range, include_groups=False).reset_index(level=0, drop=True)
            # ATR
            df['true_range'] = features['true_range']
            features['atr_14d'] = grouped['true_range'].transform(lambda x: x.rolling(14).mean())

        """ ===== æŒ‰åˆçº¦ç±»å‹æ·»åŠ ç‰¹å®šç‰¹å¾ ===== """
        if contract_type in ['CS', 'ETF']:
            """ ===== è‚¡ç¥¨/ETF ç‰¹æœ‰ç‰¹å¾ ===== """
            # é‡èƒ½ç‰¹å¾
            if 'volume' in df.columns:
                # æ»šåŠ¨å‡å€¼
                features['volume_10d_ma'] = grouped['volume'].transform(lambda x: x.rolling(10).mean())
                features['volume_ratio'] = df['volume'] / features['volume_10d_ma']
                # åŠ¨é‡
                df['volume_ratio'] = features['volume_ratio']
                features['volume_momentum'] = grouped['volume_ratio'].transform(lambda x: x - x.shift(5))

            if 'total_turnover' in df.columns:
                # æ¢æ‰‹ç‡ä¸å‡å€¼æ¯”
                features['turnover_ratio'] = grouped['total_turnover'].transform(lambda x: x / x.rolling(30).mean())

            # äº¤æ˜“æ´»è·ƒåº¦ç‰¹å¾
            if 'num_trades' in df.columns:
                features['trade_frequency'] = df['num_trades'] / df['volume']
                # 20æ—¥å‡å€¼
                df['trade_frequency'] = features['trade_frequency']
                features['trade_frequency_20d_ma'] = grouped['trade_frequency'].transform(lambda x: x.rolling(20).mean())
                features['trade_frequency_ratio'] = features['trade_frequency'] / features['trade_frequency_20d_ma']

            # å¸‚åœºçŠ¶æ€ç‰¹å¾
            if all(col in df.columns for col in ['close', 'limit_up', 'limit_down']):
                features['is_limit_up'] = (df['close'] >= df['limit_up'] * 0.995).astype(int)
                features['is_limit_down'] = (df['close'] <= df['limit_down'] * 1.005).astype(int)
                # 20æ—¥è®¡æ•°
                df['is_limit_up'] = features['is_limit_up']
                df['is_limit_down'] = features['is_limit_down']
                features['limit_up_count_20d'] = grouped['is_limit_up'].transform(lambda x: x.rolling(20).sum())
                features['limit_down_count_20d'] = grouped['is_limit_down'].transform(lambda x: x.rolling(20).sum())

            # æ¢æ‰‹ç‡ç‰¹å¾ï¼ˆè‚¡ç¥¨ç‰¹æœ‰ï¼‰ï¼šæ­¤å¤„æ¶‰åŠå¤–éƒ¨æ•°æ®ï¼Œåˆ†ç»„å¤„ç†éš¾åº¦å¤§ï¼Œä¿æŒåŸé€»è¾‘ä½†éœ€æ³¨æ„å¤–éƒ¨æ•°æ®å¯¹é½
            features['turnover_rate_approx'] = df['total_turnover'] / (df['close'] * df['volume'])
            df['turnover_rate_approx'] = features['turnover_rate_approx']

        elif contract_type == 'INDX':
            """ ===== æŒ‡æ•°ç‰¹æœ‰ç‰¹å¾ ===== """
            # å¸‚åœºå¹¿åº¦æŒ‡æ ‡
            if 'high' in df.columns and 'low' in df.columns:
                # æŒ‡æ•°æ³¢åŠ¨èŒƒå›´
                features['index_range'] = grouped[['high', 'low', 'close']].apply(
                    lambda x: (x['high'] - x['low']) / x['close'].shift(1),
                    include_groups=False
                ).reset_index(level=0, drop=True)
                # 20æ—¥å‡å€¼
                df['index_range'] = features['index_range']
                features['index_range_20d_ma'] = grouped['index_range'].transform(lambda x: x.rolling(20).mean())

            # æŒ‡æ•°åŠ¨é‡å¼ºåº¦
            features['index_momentum_strength'] = features['returns'] / features['vol_20d']

        elif contract_type in ['Future', 'Option']:
            """ ===== æœŸè´§/æœŸæƒç‰¹æœ‰ç‰¹å¾ ===== """
            # æŒä»“é‡ç‰¹å¾ï¼ˆæœŸè´§/æœŸæƒï¼‰
            if 'open_interest' in df.columns:
                # 1æ—¥/5æ—¥å˜åŒ–
                features['oi_1d_change'] = grouped['open_interest'].transform(lambda x: x.pct_change())
                features['oi_5d_change'] = grouped['open_interest'].transform(lambda x: x.pct_change(5))
                # åŠ¨é‡
                df['oi_1d_change'] = features['oi_1d_change']
                features['oi_momentum'] = grouped['oi_1d_change'].transform(lambda x: x - x.rolling(5).mean())

            settlement_col = 'settlement' if 'settlement' in df.columns else 'close'
            features['settlement'] = df[settlement_col]

            # æœŸè´§ç‰¹æœ‰ç‰¹å¾ï¼šåŸºå·®å’ŒæœŸé™ç»“æ„æ¶‰åŠå¤šä¸ªåˆçº¦çš„æ•°æ®å¯¹é½ï¼Œæ­¤å¤„ä¿æŒåŸé€»è¾‘ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ

        elif contract_type == 'Option':
            # è¡Œæƒä»·ç›¸å…³ç‰¹å¾
            if 'strike_price' in df.columns:
                features['moneyness'] = df['close'] / df['strike_price']
                # 20æ—¥å‡å€¼
                df['moneyness'] = features['moneyness']
                features['moneyness_20d_ma'] = grouped['moneyness'].transform(lambda x: x.rolling(20).mean())
                features['moneyness_deviation'] = features['moneyness'] - features['moneyness_20d_ma']
            # éšå«æ³¢åŠ¨ç‡ä¼°ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if 'strike_price' in df.columns and 'settlement' in df.columns:
                time_to_expiry = 30
                # éšå«æ³¢åŠ¨ç‡çš„è®¡ç®—ä¸æ¶‰åŠæ»šåŠ¨æˆ– shiftï¼Œä½†ä½¿ç”¨ apply ç¡®ä¿åœ¨ç»„å†…æ“ä½œ
                features['implied_vol'] = grouped[['settlement', 'strike_price']].apply(
                    lambda x: np.sqrt(2 * np.pi / time_to_expiry) * (x['settlement'] / x['strike_price']),
                    include_groups=False
                ).reset_index(level=0, drop=True)

        """ ===== æ‰€æœ‰åˆçº¦ç±»å‹é€šç”¨çš„é«˜çº§ç‰¹å¾ ===== """
        # é£é™©è°ƒæ•´æ”¶ç›Š
        # å¤æ™®æ¯”ç‡
        features['sharpe_20d'] = grouped['returns'].transform(lambda x: x.rolling(20).mean()) / features[
            'vol_20d'] * np.sqrt(252)
        df['sharpe_20d'] = features['sharpe_20d']

        # æ³¢åŠ¨ç‡çŠ¶æ€ (qcut æ˜¯å…¨å±€æ“ä½œï¼Œæ— éœ€åˆ†ç»„è®¡ç®—)
        features['vol_regime'] = pd.qcut(features['vol_20d'], q=5, labels=False, duplicates='drop') / 4
        df['vol_regime'] = features['vol_regime']

        # è¶‹åŠ¿å¼ºåº¦
        trend_window = 20
        # æ»šåŠ¨æ ‡å‡†å·®å’Œå‡å€¼
        price_std = grouped['close'].transform(lambda x: x.rolling(trend_window).std())
        price_mean = grouped['close'].transform(lambda x: x.rolling(trend_window).mean())
        features['trend_strength'] = (df['close'] - price_mean) / (price_std + 1e-10)
        df['trend_strength'] = features['trend_strength']

        # å°¾éƒ¨é£é™©æŒ‡æ ‡
        # VaR
        features['var_95'] = grouped['returns'].transform(lambda x: x.rolling(60).quantile(0.05))
        df['var_95'] = features['var_95']

        # CVaR(æ¡ä»¶é£é™©ä»·å€¼)
        df['cvar_returns_filtered'] = features['returns'].where(features['returns'] <= features['var_95'])
        features['cvar_95'] = grouped['cvar_returns_filtered'].transform(lambda x: x.rolling(60, min_periods=1).mean())   # åœ¨æ¯ä¸ªåˆçº¦åˆ†ç»„å†…ï¼Œå¯¹è¿‡æ»¤åçš„ï¼ˆç¨€ç–ï¼‰æ”¶ç›Šç‡è®¡ç®—æ»šåŠ¨å¹³å‡ã€‚
        df.drop(columns=['cvar_returns_filtered'], inplace=True)

        # å¸‚åœºçŠ¶æ€ç»¼åˆæŒ‡æ ‡ (åŸºäºå·²åˆ†ç»„è®¡ç®—çš„ç‰¹å¾ï¼Œæ— éœ€å†åˆ†ç»„)
        features['market_regime'] = (
            0.4 * features['vol_regime'] +
            0.3 * abs(features['trend_strength']) +
            0.3 * (1 - features['sharpe_20d'].clip(lower=0, upper=1))
        )

        """ ===== ç‰¹å¾å·¥ç¨‹åå¤„ç† ===== """
        MAX_ROLLING_WINDOW = settings.financial_data.features_max_rolling_window
        features = features.groupby('order_book_id').apply(
            lambda x: x.iloc[MAX_ROLLING_WINDOW:, :],
            include_groups=False
        ).reset_index(level=0, drop=False)      # æŒ‰ order_book_id åˆ†ç»„ï¼Œä¸¢å¼ƒæ¯ä¸ªåˆ†ç»„çš„å‰ MAX_ROLLING_WINDOW è¡Œ
        features = features.reset_index(drop=True)
        features = features.replace([np.inf, -np.inf], np.nan)

        # å¡«å……å¿…é¡»åœ¨åˆ†ç»„åè¿›è¡Œï¼Œä»¥é¿å…ä½¿ç”¨ä¸‹ä¸€åªè‚¡ç¥¨çš„æ•°æ®å¡«å……å‰ä¸€åªè‚¡ç¥¨çš„NaN
        features_grouped_for_fillna = features.groupby('order_book_id')
        features = features_grouped_for_fillna.apply(
            lambda x: x.fillna(method='ffill'), include_groups=False).reset_index(level=0, drop=False)   # ä¸å¯ä½¿ç”¨bfillï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        features = features.fillna(0)
        features = features.reset_index(drop=True)

        # # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åœ¨åˆç†èŒƒå›´å†… (å…¨å±€ç»Ÿè®¡æ“ä½œï¼Œä¿æŒä¸å˜)
        # for col in features.columns:
        #     if features[col].dtype in [np.float64, np.float32]:
        #         mean = features[col].mean()
        #         std = features[col].std()
        #         lower_bound = mean - 5 * std
        #         upper_bound = mean + 5 * std
        #         features[col] = features[col].clip(lower=lower_bound, upper=upper_bound)

        # ç§»é™¤å¯èƒ½ç”± apply å¼•å…¥çš„é¢å¤–ç´¢å¼•
        features = features.sort_values(['date', 'order_book_id'])
        features.to_csv(output_path, index=False)
        return output_path


if __name__ == '__main__':
    ml_service = MLService()
    cs_list = ['000001.XSHE', '000002.XSHE', '000004.XSHE']
    # print(ml_service.construct_contract_features('CS', cs_list, '20240401', '20251128'))
    print(ml_service.summarize_CSanalysis(start_date=20250401,
       end_date=20251128,
       target_stock_id='000002.XSHE',
       order_book_id_list=cs_list))
