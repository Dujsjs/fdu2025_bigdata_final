{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931e48e1-dcd6-4600-b83b-01ef44e32faa",
   "metadata": {},
   "source": [
    "# GPU机器模型测试\n",
    "\n",
    "- https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct\n",
    "  - context 128K \n",
    "  - 生成token 8K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657c0200-b8b7-496e-9ab6-de1a2743bf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081585817ff64306b9a1b6dbcc77bdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/nas-private/huggingface_cache/Qwen25/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af4c8ee-16c6-4788-b914-8141065a9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Certainly! A Large Language Model (LLM) is a type of artificial intelligence system designed to understand and generate human-like text based on the input it receives. These models are typically deep learning models that have been trained on vast amounts of text data from the internet, books, and other sources. They can perform a wide range of natural language processing tasks, including but not limited to:\n",
      "\n",
      "- **Text Generation**: Creating coherent paragraphs or even entire documents.\n",
      "- **Translation**: Converting text from one language to another.\n",
      "- **Summarization**: Condensing long texts into shorter summaries.\n",
      "- **Question Answering**: Providing answers to questions based on given information.\n",
      "- **Dialogue Generation**: Engaging in conversations with users.\n",
      "\n",
      "LLMs are usually based on transformer architectures, which allow them to understand context and generate responses that are relevant and meaningful. The training process involves learning patterns and relationships within the text data, enabling the model to generate text that is not only grammatically correct but also semantically coherent.\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False \n",
    "    # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f55ce2-f2d2-424c-b535-9501f910ff31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
