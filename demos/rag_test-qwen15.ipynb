{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 内存已尝试释放。\n",
      "当前已分配显存: 0.00 GB\n",
      "当前缓存中显存: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"当前已分配显存: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"当前缓存中显存: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"无GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/nas-private/huggingface_cache/Qwen/Qwen1.5-7B-Chat /root/nas-private/huggingface_cache/BAAI/bge-small-zh-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4b049fbd714644a30056f7fcd6aec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# 定义 max_memory 参数\n",
    "# 键是设备编号 (0 代表第一张 GPU)，值是内存大小\n",
    "# 你可以使用 \"GiB\", \"MiB\", \"GB\", \"MB\" 等单位\n",
    "max_memory_map = {0: \"15GiB\"} \n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# 定义模型根目录\n",
    "HF_CACHE_DIR = \"/root/nas-private/huggingface_cache\"\n",
    "# 模型的相对路径（不带开头的 ./，保持路径结构清晰）\n",
    "LLM_RELATIVE_PATH = \"Qwen/Qwen1.5-7B-Chat\" \n",
    "#LLM_RELATIVE_PATH = \"Qwen/Qwen3-8b\" \n",
    "EMBED_RELATIVE_PATH = \"BAAI/bge-small-zh-v1.5\" \n",
    "# 构造完整的本地路径\n",
    "# 关键：确保这个路径是模型文件（如 config.json）所在文件夹的路径\n",
    "LOCAL_LLM_PATH = os.path.join(HF_CACHE_DIR, LLM_RELATIVE_PATH)\n",
    "LOCAL_EMBED_PATH =os.path.join(HF_CACHE_DIR,EMBED_RELATIVE_PATH)\n",
    "\n",
    "print(LOCAL_LLM_PATH,LOCAL_EMBED_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_PATH, trust_remote_code=True)\n",
    "llm = HuggingFaceLLM(    # Qwen 1.5 default   context_window=3900, num_output=256\n",
    "    #context_window=8192,  # 上下文窗口大小\n",
    "    #max_new_tokens=2048,   # 最大生成长度\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=LOCAL_LLM_PATH, # 注意这里改为本地路径\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        LOCAL_LLM_PATH,        # 注意这里改为本地路径\n",
    "        device_map=\"auto\",             # 自动分配到 GPU (3090)\n",
    "       # torch_dtype=\"auto\",  old\n",
    "       dtype=\"auto\",  #torch.float16,     # 使用 float16 减少显存占用\n",
    "      #  load_in_4bit=True,             # 使用 4-bit 量化\n",
    "        trust_remote_code=True,\n",
    "        max_memory=max_memory_map\n",
    "    )\n",
    ")\n",
    "# 初始化本地 Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=LOCAL_EMBED_PATH, # 注意这里改为本地路径\n",
    ")\n",
    "# 设置嵌入模型\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 设置LLM模型\n",
    "Settings.llm =llm\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型生成句子嵌入# 本练习不需要\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "model = SentenceTransformer(LOCAL_EMBED_PATH)\n",
    "\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings),len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 complete 模式\n",
    "\n",
    "本模型不支持，因为是chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Response (Complete) ---\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这种方法只适用于单次、无历史的提示\n",
    "prompt = \"写一个关于中国长城的小故事，不超过100字。\"\n",
    "\n",
    "# 使用 .complete() 方法（如果您的 HuggingFaceLLM 实例支持）\n",
    "# 注意：通常 chat() 是推荐用于与 Chat 模型交互的方式，但 complete() 也可以用于简单的提示\n",
    "try:\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    print(\"--- LLM Response (Complete) ---\")\n",
    "    print(response.text)\n",
    "    print(\"-------------------------------\\n\")\n",
    "except AttributeError:\n",
    "    print(\"Settings.llm 实例不支持 .complete() 方法，请使用包装后的 .chat()。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 chat 模式\n",
    "\n",
    "### 2.2.1 一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: 写一个关于中国长城的小故事，不超过100字。\n",
      "\n",
      "--- LLM Response (Chat) ---\n",
      "明朝年间，小明是个好奇的孩子。他听说那遥不可及的城墙能抵御外敌，就决定一探究竟。历经艰辛，他爬上长城，眼前一片雄伟。风中，他仿佛听到历史的回响，小小的心被长城的坚韧与勇气深深打动。从那以后，小明明白了，这不仅仅是一堵墙，它是中华儿女的骄傲。\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入 ChatMessage，用于构造对话历史\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import Settings # 确保 Settings 在当前作用域可用\n",
    "\n",
    "# 假设您想问一个关于中国的简单问题\n",
    "prompt_content = \"写一个关于中国长城的小故事，不超过100字。\"\n",
    "\n",
    "print(f\"User Prompt: {prompt_content}\\n\")\n",
    "\n",
    "# **关键修改：将字符串包装成 ChatMessage 列表**\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.USER, content=prompt_content)\n",
    "]\n",
    "\n",
    "# 直接调用 llm 实例的 chat 方法，传入 messages 列表\n",
    "response = Settings.llm.chat(messages)\n",
    "\n",
    "print(\"--- LLM Response (Chat) ---\")\n",
    "print(response.message.content)\n",
    "print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Engine Conversation ---\n",
      "Assistant: 《围城》。这是钱钟书先生的经典之作，以讽刺和幽默的手法描绘了上世纪30年代中国社会的各种面貌，秋天读可以感受到季节的变迁，同时小说中的人生哲理也耐人寻味。\n",
      "\n",
      "Assistant: 当然，钱钟书是中国现代文学的重要作家，除了《围城》，他的代表作还包括《管锥编》和《谈艺录》等，这些都是学术性和文化修养极高的著作。另外，他的妻子杨绛也有不少知名作品，如《洗澡》和《我们仨》等。\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "# 使用 Settings 中配置的 llm 创建 ChatEngine\n",
    "chat_engine = SimpleChatEngine.from_defaults(\n",
    "    llm=Settings.llm,\n",
    "    # System Prompt 可选，用于设定模型角色\n",
    "    system_prompt=\"你是一个乐于助人、充满好奇心的 AI 助手，你的回答总是简洁明了。\",\n",
    ")\n",
    "\n",
    "print(\"--- Chat Engine Conversation ---\")\n",
    "\n",
    "# 第一次提问\n",
    "response_1 = chat_engine.chat(\"给我推荐一个适合在秋天阅读的中文小说名字。\")\n",
    "print(f\"Assistant: {response_1.response}\\n\")\n",
    "\n",
    "# 第二次提问（ChatEngine 会自动维护历史）\n",
    "response_2 = chat_engine.chat(\"这个作者还有其他的作品吗？\")\n",
    "print(f\"Assistant: {response_2.response}\\n\")\n",
    "\n",
    "print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 工具调用测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMMetadata(context_window=8192, num_output=2048, is_chat_model=False, is_function_calling_model=False, model_name='/root/nas-private/huggingface_cache/Qwen3', system_role=<MessageRole.SYSTEM: 'system'>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 异步调用 1: 工具函数 (乘法) ---\n",
      "最终回答: assistant: 5 乘以 9 的结果是 145。\n",
      "\n",
      "--- 异步调用 2: 工具函数 (天气) ---\n",
      "最终回答: assistant: 上海今天是-8°C，下雨，空气质量一般。记得出门带伞哦。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core import Settings # 您的 LLM 在 Settings.llm 中\n",
    "# ... (您的 LLM 和 Settings.llm 配置已完成) ...\n",
    "\n",
    "# ----------------- 定义工具 (假设已成功) -----------------\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"获取指定城市（例如：'北京'，'上海'）的当前天气信息。\"\"\"\n",
    "    if \"北京\" in city:\n",
    "        return \"北京今天的气温是 28°C，多云转晴，空气质量良好。\"\n",
    "    elif \"上海\" in city:\n",
    "        return \"上海今天的气温是 -8°C，下雨，空气质量一般。\"\n",
    "    return f\"抱歉，没有 {city} 的天气数据。\"\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"将两个整数相乘。\"\"\"\n",
    "    return a * b+100\n",
    "\n",
    "weather_tool = FunctionTool.from_defaults(fn=get_current_weather)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "tools = [weather_tool, multiply_tool]\n",
    "\n",
    "\n",
    "agent = ReActAgent(tools=tools, llm=Settings.llm, verbose=True)\n",
    "\n",
    "# ----------------- 定义异步执行函数 -----------------\n",
    "async def run_agent_queries(agent_instance: ReActAgent):\n",
    "    print(\"--- 异步调用 1: 工具函数 (乘法) ---\")\n",
    "    agent_query_calc = \"请计算一下 5 乘以 9 的结果是多少？\"\n",
    "    # 使用 .arun() 方法进行异步执行\n",
    "    response_calc = await agent_instance.run(agent_query_calc)\n",
    "    print(f\"最终回答: {response_calc.response}\\n\")\n",
    "\n",
    "    print(\"--- 异步调用 2: 工具函数 (天气) ---\")\n",
    "    agent_query_weather = \"告诉我上海今天的天气怎么样？\"\n",
    "    response_weather = await agent_instance.run(agent_query_weather)\n",
    "    print(f\"最终回答: {response_weather.response}\\n\")\n",
    "    \n",
    "await run_agent_queries(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邓国标的工作单位是XTransfer（上海夺畅网络技术有限公司），担任CEO兼联合创始人。他的复旦教育经历是在1998年参加的上海市高级管理干部培训班（百人工程）第五期，专业是高端培训。\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"./txt\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"邓国标工作单位是什么，复旦教育经历的是什么\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=index.as_query_engine(llm=Settings.llm).query(\"用中文回答，施小琳教育经历\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下内容与施小琳有关\\n姓名：施小琳\\n性别：女\\n婚姻状况\\t其他\\n居住地\\t上海市\\n更新时间\\t2023/03/01\\n施小琳标签\\t两会代表, 院长级, 二十大人大代表, 第十四届全国人大代表（2023）\\n施小琳项目\\t政府培训班\\n\\n施小琳教育经历：复旦管院政府培训班/1998上海市高级管理干部培训班(百人工程)第五期/高端培训\\t\\n\\n默认邮箱\\tshi_xiaolin@hotmail.com\\n\\n施小琳当前工作(工作单位/部门/职位)：四川省省委/副书记;四川省政府/党组书记;四川省人民政府/省长\\n\\n行业\\t政府机构/非营利机构;政府机构/非营利机构;政府机构/非营利机构\\n\\n二类行业\\t政府机构/非营利机构;政府机构/非营利机构;政府机构/非营利机构\\n\\n---\\n\\n施小琳\\n工作履历(工作单位/部门/职位)\\t上海市普陀区人民政府/区委书记、区委委员;上海市民政局/局长;南汇区人民政府/副区长;虹口区人民政府/副区长;上海市委常委/统战部部长;江西省委/常委;江西省宣传部/部长;四川省成都市/成都市委书记、兼任成都警备区党委第一书记;四川省人民政府/副省长、代理省长\\n\\n政府类职级\\t省部级副职;省部级副职;省部级副职;省部级副职;省部级正职\\n\\n人际关系(是否校友/姓名/关系/公司/职位/手机号/邮箱/项目班级)\\t非校友/陈叶丹/无/无/无/13818353646/无/无\\n\\n# 施小琳职务变更\\n## \\t2024/07/31 施小琳任四川省 省长\\t成都发布\\t\\n\\n## \\t2024/07/04 施小琳任四川省副省长、代理省长\\t\\t澎湃新闻\\t\\n\\n## \\t施小琳已任四川省政府党组书记\\t2024/06/29\\t澎湃新闻\\t\\n\\n## \\t成都市委书记施小琳履新四川省委副书记\\t2023/07/03\\t人民网\\n\\n# 人大代表\\n中国共产党第二十次全国代表大会代表名单\\t2022/10/16\\t新华社\\n\\n---\\n\\n施小琳有关新闻\\n\\n新华社成都2024年7月31日电\\u2003四川省第十四届人民代表大会第三次会议7月31日选举施小琳为四川省人民政府省长。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.source_nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "text_qa_template_str = (\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\n only Using\"\n",
    "    \" the context information, not using your own knowledge, answer\"\n",
    "    \" the question: {query_str}\\nIf the context isn't helpful, you will not \"\n",
    "    \" answer the question on your own.\\n 所有回答以中文显示 \\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\nWe have provided an\"\n",
    "    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n",
    "    \" the existing answer (only if needed) with some more context\"\n",
    "    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing the new\"\n",
    "    \" context , update or repeat the existing answer.\\n 所有回答以中文显示 \\n\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    index.as_query_engine(\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        llm=llm,\n",
    "    ).query(\"按时间顺序，列出施小琳的所有职务变更，如果有时间也列出来。\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
