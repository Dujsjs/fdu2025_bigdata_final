{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d26b047-acce-43ac-8cee-3bb79151fdc9",
   "metadata": {},
   "source": [
    "# GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6992fef9-b4f8-4b98-a7a2-69f8fcb84955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前已分配显存: 0.00 GB\n",
      "当前缓存中显存: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"当前已分配显存: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"当前缓存中显存: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af1b46-4399-48b1-9537-4df47db9bd3e",
   "metadata": {},
   "source": [
    "# 1 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ee7c2d-b96c-474d-9acc-dac3fec7cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# 定义 max_memory 参数\n",
    "# 键是设备编号 (0 代表第一张 GPU)，值是内存大小\n",
    "# 你可以使用 \"GiB\", \"MiB\", \"GB\", \"MB\" 等单位\n",
    "max_memory_map = {0: \"15GiB\"} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf06407-8756-48de-a255-518e9674186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型和分词器...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7043ece02b4ca1a3330c0a35fefc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型和分词器加载完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# --- 1. 保持您原有的模型和分词器加载方式 ---\n",
    "# 这部分完全不变，因为它正确地使用了 transformers 的功能来加载模型到 GPU\n",
    "print(\"正在加载模型和分词器...\")\n",
    "HF_CACHE_DIR = \"/root/nas-private/bigdata_final_project/models/huggingface_cache\"\n",
    "LLM_RELATIVE_PATH = \"/root/nas-private/bigdata_final_project/models/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "LOCAL_LLM_PATH = os.path.join(HF_CACHE_DIR, LLM_RELATIVE_PATH)\n",
    "model_name = LOCAL_LLM_PATH\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 确保在加载模型时处理可能的信任远程代码问题\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    max_memory=max_memory_map,\n",
    "    #torch_dtype=\"auto\", # 自动选择最佳精度 (如 bfloat16)\n",
    "    dtype= \"auto\",\n",
    "    device_map=\"auto\",  # 智能地将模型分布到所有可用 GPU\n",
    "    trust_remote_code=True # Qwen 系列模型通常需要这个\n",
    ")\n",
    "print(\"模型和分词器加载完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56c7dcd-b72c-48e3-b8b3-f75d8bcab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在将模型包装到 LlamaIndex 的 HuggingFaceLLM 中...\n",
      "HuggingFaceLLM 包装完成。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. 将加载好的对象包装进 HuggingFaceLLM ---\n",
    "# 这是关键的改写步骤\n",
    "print(\"正在将模型包装到 LlamaIndex 的 HuggingFaceLLM 中...\")\n",
    "\n",
    "# 创建 HuggingFaceLLM 实例\n",
    "llm = HuggingFaceLLM(\n",
    "    # 传入已经加载好的模型和分词器对象\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    # 提供模型名称，用于元数据记录\n",
    "    model_name=model_name,\n",
    "    \n",
    "    # 关键参数：告知 LlamaIndex 模型的上下文窗口大小\n",
    "    # Qwen3-8B 的上下文窗口非常大，这里设置为一个合理的值，例如 32768\n",
    "    context_window= 8192, #32768,\n",
    "    \n",
    "    # 控制生成文本的最大长度\n",
    "    max_new_tokens=1024,   #512,\n",
    "    # 传递给 transformers.generate() 的额外参数\n",
    "    generate_kwargs={\"do_sample\": True, \"temperature\": 0.7, \"top_k\": 50},\n",
    "    \n",
    "    # 告知包装器模型已经通过 device_map 分布在设备上\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"HuggingFaceLLM 包装完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3763c978-4fe0-4ee8-ac34-1d55f0f4f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# 模型的相对路径（不带开头的 ./，保持路径结构清晰）\n",
    "EMBED_RELATIVE_PATH = \"/root/nas-private/bigdata_final_project/models/bge-small-zh-v1.5\"\n",
    "LOCAL_EMBED_PATH =os.path.join(HF_CACHE_DIR,EMBED_RELATIVE_PATH)\n",
    "# 初始化本地 Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=LOCAL_EMBED_PATH, # 注意这里改为本地路径\n",
    ")\n",
    "\n",
    "# 设置嵌入模型\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 设置LLM模型\n",
    "Settings.llm =llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8492dfac-5f73-4a9e-8dbf-0532d84d8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT_TEMPLATE_STR = (\n",
    "    \"You are a helpful assistant. Please answer the user's question based on the provided context.\\n\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db23243-0449-407d-8e63-c24ff69df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, PromptTemplate\n",
    "qa_prompt_template = PromptTemplate(QA_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3bcde-6608-4831-81a6-b6a6a71c5347",
   "metadata": {},
   "source": [
    "# 2  chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb758670-db0a-4924-8b30-46f599c0e2e9",
   "metadata": {},
   "source": [
    "## 2.1 complete 模式\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d0933b-420b-4d89-937c-a4b7346a62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Response (Complete) ---\n",
      " 从前，皇帝为抵御北方的入侵者，命令无数工匠和士兵修建长城。一位年轻的石匠在修筑长城时，与邻近村庄的姑娘相爱了。但战争不断，姑娘的父亲要求石匠离开。石匠将自己雕刻的一块心形石块留在城墙上，以示对姑娘的思念。多年后，战争结束，石匠回到家乡，找到了那块心形石块，重新与心爱的人团聚。这象征着长城不仅是防御工事，也是连接人心的桥梁。\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这种方法只适用于单次、无历史的提示\n",
    "prompt = \"写一个关于中国长城的小故事，不超过100字。\"\n",
    "\n",
    "# 使用 .complete() 方法（如果您的 HuggingFaceLLM 实例支持）\n",
    "# 注意：通常 chat() 是推荐用于与 Chat 模型交互的方式，但 complete() 也可以用于简单的提示\n",
    "try:\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    print(\"--- LLM Response (Complete) ---\")\n",
    "    print(response.text)\n",
    "    print(\"-------------------------------\\n\")\n",
    "except AttributeError:\n",
    "    print(\"Settings.llm 实例不支持 .complete() 方法，请使用包装后的 .chat()。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f7708-2acf-4ee9-ae3d-c0119f7b9071",
   "metadata": {},
   "source": [
    "## 2.2 chat 模式\n",
    "\n",
    "### 2.2.1 一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5fa523-6e33-44f6-9a5f-75541de4a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: 写一个关于中国长城的小故事，不超过100字。\n",
      "\n",
      "--- LLM Response (Chat) ---\n",
      "在遥远的明朝，小村落里流传着长城能保佑平安的传说。一天，村中少年李明偶得一块刻有神秘符号的石片，得知只有将它投入长城的最北端，才能真正守护家园。于是，他踏上寻石之旅，历经千辛万苦终于抵达。当石片投入城砖间那一刻，夕阳洒落，长城仿佛被赋予了新的生命力，更显巍峨壮丽。\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入 ChatMessage，用于构造对话历史\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import Settings # 确保 Settings 在当前作用域可用\n",
    "\n",
    "# 假设您想问一个关于中国的简单问题\n",
    "prompt_content = \"写一个关于中国长城的小故事，不超过100字。\"\n",
    "\n",
    "print(f\"User Prompt: {prompt_content}\\n\")\n",
    "\n",
    "# **关键修改：将字符串包装成 ChatMessage 列表**\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.USER, content=prompt_content)\n",
    "]\n",
    "\n",
    "# 直接调用 llm 实例的 chat 方法，传入 messages 列表\n",
    "response = Settings.llm.chat(messages)\n",
    "\n",
    "print(\"--- LLM Response (Chat) ---\")\n",
    "print(response.message.content)\n",
    "print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c9db3-ef75-4456-bcc8-982b4c3d8b1a",
   "metadata": {},
   "source": [
    "### 2.2.2 多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f93fce4-42f4-491a-a554-d359ad395813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Engine Conversation ---\n",
      "Assistant: 《白鹿原》是一部非常适合秋天阅读的小说，它描绘了中国农村的生活变迁，文字细腻而富有诗意，非常适合在这个收获的季节细细品味。\n",
      "\n",
      "Assistant: 是的，陈忠实除了《白鹿原》之外，还写有《信任》《初夏》等短篇小说集，以及散文集《陈忠实自选集》等。如果你喜欢《白鹿原》，可以尝试阅读他的其他作品。\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "# 使用 Settings 中配置的 llm 创建 ChatEngine\n",
    "chat_engine = SimpleChatEngine.from_defaults(\n",
    "    llm=Settings.llm,\n",
    "    # System Prompt 可选，用于设定模型角色\n",
    "    system_prompt=\"你是一个乐于助人、充满好奇心的 AI 助手，你的回答总是简洁明了。\",\n",
    ")\n",
    "\n",
    "print(\"--- Chat Engine Conversation ---\")\n",
    "\n",
    "# 第一次提问\n",
    "response_1 = chat_engine.chat(\"给我推荐一个适合在秋天阅读的中文小说名字。\")\n",
    "print(f\"Assistant: {response_1.response}\\n\")\n",
    "\n",
    "# 第二次提问（ChatEngine 会自动维护历史）\n",
    "response_2 = chat_engine.chat(\"这个作者还有其他的作品吗？\")\n",
    "print(f\"Assistant: {response_2.response}\\n\")\n",
    "\n",
    "print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f815c9-390e-49c9-ac27-d11ad598a890",
   "metadata": {},
   "source": [
    "## 3 工具调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69274bf-f477-4b2b-8799-d7c9a76801de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMMetadata(context_window=8192, num_output=1024, is_chat_model=False, is_function_calling_model=False, model_name='/root/nas-private/bigdata_final_project/models/Qwen2.5-7B-Instruct', system_role=<MessageRole.SYSTEM: 'system'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0853c05e-8c92-4069-b1a8-d5be68a51654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core import Settings # 您的 LLM 在 Settings.llm 中\n",
    "# ... (您的 LLM 和 Settings.llm 配置已完成) ...\n",
    "\n",
    "# ----------------- 定义工具 (假设已成功) -----------------\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"获取指定城市（例如：'北京'，'上海'）的当前天气信息。\"\"\"\n",
    "    if \"北京\" in city:\n",
    "        return \"北京今天的气温是 28°C，多云转晴，空气质量良好。\"\n",
    "    elif \"上海\" in city:\n",
    "        return \"上海今天的气温是 -18°C，下雨，空气质量一般。\"\n",
    "    return f\"抱歉，没有 {city} 的天气数据。\"\n",
    "\n",
    "def multiply(a:int, b:int ) -> int :\n",
    "    \"\"\"将两个数相乘(乘法运算)\"\"\"\n",
    "    return a * b + 10000\n",
    "\n",
    "weather_tool = FunctionTool.from_defaults(fn=get_current_weather)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "tools = [weather_tool, multiply_tool]\n",
    "\n",
    "\n",
    "agent = ReActAgent(tools=tools, llm=Settings.llm, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53343b35-c188-4af0-960a-f80460f680af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 异步调用 1: 工具函数 (乘法) ---\n",
      "最终回答: assistant: 50 乘以 91 的结果是 14550。\n",
      "\n",
      "--- 异步调用 2: 工具函数 (天气) ---\n",
      "最终回答: assistant: 上海今天的天气情况是气温 -18°C，并且有雨，空气质量一般。请注意保暖并关注实时天气变化。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------- 定义异步执行函数 -----------------\n",
    "async def run_agent_queries(agent_instance: ReActAgent):\n",
    "    print(\"--- 异步调用 1: 工具函数 (乘法) ---\")\n",
    "    agent_query_calc = \"调用乘法工具，计算一下 50 乘以 91 的结果是多少？\"\n",
    "    # 使用 .arun() 方法进行异步执行\n",
    "    response_calc = await agent_instance.run(agent_query_calc)\n",
    "    print(f\"最终回答: {response_calc.response}\\n\")\n",
    "\n",
    "    print(\"--- 异步调用 2: 工具函数 (天气) ---\")\n",
    "    agent_query_weather = \"告诉我上海今天的天气怎么样？\"\n",
    "    response_weather = await agent_instance.run(agent_query_weather)\n",
    "    print(f\"最终回答: {response_weather.response}\\n\")\n",
    "    \n",
    "await run_agent_queries(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451ccf3-2c71-44b7-947f-d729c631b0b4",
   "metadata": {},
   "source": [
    "# Rag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7abce2ee-958d-4acd-b18c-29aca55d3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "依据提供的信息，邓国标的信息并未出现在文本中，因此无法回答其工作单位和复旦教育经历。文本中提到的校友是常怀春和施小琳，其中常怀春的工作单位是“山东华鲁恒升化工股份有限公司”，复旦教育经历为“复旦EMBA;班级：2008秋1班”。而施小琳的工作单位则为“四川省省委/副书记;四川省政府/党组书记;四川省人民政府/省长”。 对于邓国标的相关信息，需要提供相关文件或数据以进行准确回答。 根据提供的信息，答案如下：\n",
      "\n",
      "- 常怀春的工作单位是山东华鲁恒升化工股份有限公司。\n",
      "- 常怀春的复旦教育经历是复旦EMBA，班级为2008秋1班。 没有提到邓国标的相关信息。 施小琳的工作单位是四川省省委/副书记; 四川省政府/党组书记; 四川省人民政府/省长。 施小琳的复旦教育经历为复旦管院政府培训班。 对于邓国标的信息，无法从给定的内容中获取。 需要更多关于邓国标的资料来进行回答。 \n",
      "\n",
      "请注意，上述答案基于提供的信息。如果需要更详细的信息，建议查找相关的详细资料。如果有更多的文档或者数据，请提供，以便给出更准确的答案。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "documents = SimpleDirectoryReader(\"/root/nas-private/bigdata_final_project/demos/txt1\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "  text_qa_template=qa_prompt_template    \n",
    ")\n",
    "response = query_engine.query(  \"邓国标工作单位是什么，复旦教育经历的是什么\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ba0b30-64e0-4be7-bf07-3525ee9899fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=index.as_query_engine(llm=Settings.llm,text_qa_template=qa_prompt_template).query(\"用中文回答，施小琳教育经历\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61562e6b-5300-453f-8072-9317177b1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是施小琳的教育经历：\n",
      "\n",
      "- 复旦管院政府培训班（1998年）\n",
      "- 上海市高级管理干部培训班（百人工程）第五期\n",
      "- 高端培训\n",
      "\n",
      "这些信息显示了她在政府管理和高级管理方面的培训背景。\n"
     ]
    }
   ],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "491fb2b7-12c3-4bae-ab4c-8a52e0c17f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f13867-cd54-414f-98e9-bfb87f59d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下内容与施小琳有关\\n姓名：施小琳\\n性别：女\\n婚姻状况\\t其他\\n居住地\\t上海市\\n更新时间\\t2023/03/01\\n施小琳标签\\t两会代表, 院长级, 二十大人大代表, 第十四届全国人大代表（2023）\\n施小琳项目\\t政府培训班\\n\\n施小琳教育经历：复旦管院政府培训班/1998上海市高级管理干部培训班(百人工程)第五期/高端培训\\t\\n\\n施小琳当前工作(工作单位/部门/职位)：四川省省委/副书记;四川省政府/党组书记;四川省人民政府/省长\\n\\n行业\\t政府机构/非营利机构;政府机构/非营利机构;政府机构/非营利机构\\n\\n二类行业\\t政府机构/非营利机构;政府机构/非营利机构;政府机构/非营利机构\\n\\n---\\n\\n施小琳\\n工作履历(工作单位/部门/职位)\\t上海市普陀区人民政府/区委书记、区委委员;上海市民政局/局长;南汇区人民政府/副区长;虹口区人民政府/副区长;上海市委常委/统战部部长;江西省委/常委;江西省宣传部/部长;四川省成都市/成都市委书记、兼任成都警备区党委第一书记;四川省人民政府/副省长、代理省长\\n\\n政府类职级\\t省部级副职;省部级副职;省部级副职;省部级副职;省部级正职'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.source_nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4155727c-35a9-4d69-ad74-4dd4ffed69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "text_qa_template_str = (\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\n only Using\"\n",
    "    \" the context information, not using your own knowledge, answer\"\n",
    "    \" the question: {query_str}\\nIf the context isn't helpful, you will not \"\n",
    "    \" answer the question on your own.\\n 所有回答以中文显示 \\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\nWe have provided an\"\n",
    "    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n",
    "    \" the existing answer (only if needed) with some more context\"\n",
    "    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing the new\"\n",
    "    \" context , update or repeat the existing answer.\\n 所有回答以中文显示 \\n\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32953cf-b03d-48f3-923d-b0a62385cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，按时间顺序列出施小琳的所有职务变更如下：\n",
      "\n",
      "1. 1990年7月参加工作\n",
      "2. 上海市民政局局长、党组书记\n",
      "3. 上海市普陀区委书记\n",
      "4. 上海市委常委、统战部部长\n",
      "5. 江西省委常委、宣传部部长\n",
      "6. 四川省委常委、成都市委书记\n",
      "7. 2023年7月，任四川省委副书记\n",
      "8. 2024年7月4日，任四川省副省长、代理省长\n",
      "9. 2024年7月31日，任四川省人民政府省长\n",
      "10. 2024年6月28日，任四川省政府党组书记\n",
      "\n",
      "注意：具体参加工作的年份未明确给出，因此将其列为最早的时间点。其余职务变更的时间均依据提供的信息。\n"
     ]
    }
   ],
   "source": [
    "res=index.as_query_engine(\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        llm=llm,\n",
    "    ).query(\"按时间顺序，列出施小琳的所有职务变更，如果有时间也列出来。\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c005305-0786-4d32-ad71-687133c6d1b1",
   "metadata": {},
   "source": [
    "# Rag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b85a6ba-5149-456d-b804-930ca5f617e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ed65ae-e515-45e1-abef-18d29177d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "documents = SimpleDirectoryReader(\"/root/nas-private/bigdata_final_project/demos/txt2\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "  text_qa_template=qa_prompt_template    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38152df2-db77-40b1-b795-5e1e70da961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融 Agent 创建完成，拥有 RAG 和 ML 分析能力。\n"
     ]
    }
   ],
   "source": [
    "# 2. 封装为 QueryEngineTool\n",
    "rag_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"financial_principle_knowledge\",\n",
    "        description=(\n",
    "            \"当用户询问**基础投资原则、风险等级、市场分类**等知识时，使用此工具。 \"\n",
    "            \"它提供来自内部金融文档的背景信息和规则。\"\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# --- B. ML/量化分析工具：模拟模型预测 ---\n",
    "\n",
    "def conduct_ml_analysis(asset_type: str, risk_level: str) -> str:\n",
    "    \"\"\"\n",
    "    调用金融机器学习模型（例如，风险预测、收益率模拟），分析特定资产在特定风险下的潜在表现。\n",
    "    输入 asset_type 必须是 '股票', '债券', '基金' 中的一种。\n",
    "    输入 risk_level 必须是 '保守', '稳健', '激进' 中的一种。\n",
    "    返回分析报告的摘要。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 模拟复杂的模型运行结果\n",
    "    if risk_level == '激进' and asset_type == '股票':\n",
    "        result = \"机器学习模型预测：高成长科技股在未来12个月内有 75% 的概率获得 20% 以上的年化收益，但其最大回撤风险为 35%。建议严格控制仓位。\"\n",
    "    elif risk_level == '保守' and asset_type == '债券':\n",
    "        result = \"模型分析：高等级企业债券在当前市场环境下流动性较好，预计年化收益在 4%-6% 之间，风险极低，适合保值。\"\n",
    "    elif risk_level == '稳健' and asset_type == '基金':\n",
    "        result = \"模型预测：平衡型混合基金在当前波动市场中表现稳健，模型给予‘增持’评级，建议配置 40% 的资产。\"\n",
    "    else:\n",
    "        result = f\"模型对 {risk_level} 投资者配置 {asset_type} 的分析结果暂无高置信度报告，建议结合RAG知识库中的原则进行配置。\"\n",
    "        \n",
    "    return result\n",
    "\n",
    "# 封装为 FunctionTool\n",
    "ml_analysis_tool = FunctionTool.from_defaults(fn=conduct_ml_analysis)\n",
    "\n",
    "\n",
    "# --- C. 构建 ReAct Agent ---\n",
    "agent = ReActAgent(\n",
    "    tools=[rag_tool, ml_analysis_tool],\n",
    "    llm=Settings.llm,\n",
    "    verbose=True, # 开启 verbose，观察 Qwen 的思考过程\n",
    "    # 设置一个合适的系统提示，引导 Qwen 扮演金融顾问的角色\n",
    "    system_prompt=\"你是一位专业的金融投资顾问。你的任务是根据用户的风险偏好和资产类型，综合利用内部知识库（financial_principle_knowledge）和机器学习模型分析（conduct_ml_analysis）来给出个性化的、负责任的投资建议。\",\n",
    ")\n",
    "print(\"金融 Agent 创建完成，拥有 RAG 和 ML 分析能力。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31006ea2-2091-4098-a7fe-9a1e90a4d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_finance_queries(agent_instance: ReActAgent,query:str):\n",
    "    #print(f\"\\n[ Agent Query : {query} ]\")\n",
    "    response = await agent_instance.run(query)\n",
    "    #print(\"\\n\" + \"=\"*50)\n",
    "    #print(f\"Agent 最终回答 A:\\n{response.response}\") \n",
    "    #print(\"=\"*50 + \"\\n\")\n",
    "    return response.response\n",
    "async def ask(q):\n",
    "    # 必须使用 await 调用异步函数\n",
    "    r = await run_finance_queries(agent, q)\n",
    "    return r.blocks[0].text\n",
    "    \n",
    "q_string=\"我是一个激进型投资者，请问基于内部文档的原则和机器学习模型分析，我应该如何配置高成长性股票？\"    \n",
    "res= await ask(q_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39d56736-a554-4175-b4ba-2c8088a034af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'激进型投资者可以根据以下建议来配置高成长性股票：\\n\\n1. 将超过50%的资金配置到高成长性股票上，特别是高成长科技股。\\n2. 重点关注具有较强增长潜力的股票，尤其是那些在未来12个月内有75%的概率获得20%以上年化收益的股票。\\n3. 注意控制仓位，因为这些股票的最大回撤风险为35%，需要严格控制风险。\\n\\n总之，激进型投资者应该积极寻找高增长潜力的股票进行投资，但在投资过程中需要密切关注市场动态，及时调整投资组合，并严格控制仓位以降低风险。希望以上建议对您有所帮助。如果有任何疑问或需要进一步的信息，请随时联系我。祝您投资顺利！'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca1609b-326d-4109-ae0d-2877adbaf7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对不起，我目前没有查询天气的功能。你可以尝试使用其他天气查询服务来获取明天的天气预报。\n"
     ]
    }
   ],
   "source": [
    "q_string=\"明天的天气？\"    \n",
    "res= await ask(q_string)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2ff7f-5375-4283-9b71-3659d8405730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}